% This file was created with Citavi 6.10.0.0

@book{.,
 author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
 title = {Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville},
 file = {Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville (z-lib.org):Attachments/Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville (z-lib.org).pdf:application/pdf}
}


@book{.i,
 author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
 title = {The Elements of Statistical Learning},
 file = {statistical{\_}learning:Attachments/statistical{\_}learning.pdf:application/pdf}
}


@proceedings{AISTATS.2010,
 year = {2010},
 title = {AISTATS}
}


@proceedings{AISTATS.2019,
 year = {2019},
 title = {AISTATS}
}


@proceedings{AISTATS.2020,
 year = {2020},
 title = {AISTATS}
}


@proceedings{AISTATS.2021,
 year = {2021},
 title = {AISTATS}
}


@inproceedings{Alaa.2017,
 abstract = {Neural Information Processing Systems http://nips.cc/},
 author = {Alaa, Ahmed M. and {van der Schaar}, Mihaela},
 title = {Bayesian inference of individualized treatment effects using multi-task {G}aussian processes},
 booktitle = {NeurIPS},
 year = {2017},
 file = {NIPS-2017-bayesian-inference-of-individualized-treatment-effects-using-multi-task-gaussian-processes-Paper:Attachments/NIPS-2017-bayesian-inference-of-individualized-treatment-effects-using-multi-task-gaussian-processes-Paper.pdf:application/pdf}
}


@inproceedings{Alaa.2018,
 abstract = {Limits of Estimating Heterogeneous Treatment Effects: Guidelines for Practical Algorithm DesignAhmed Alaa,~Mihaela SchaarEstimating heterogeneous treatm...},
 author = {Alaa, Ahmed M. and {van der Schaar}, Mihaela},
 title = {Limits of estimating heterogeneous treatment effects: Guidelines for practical algorithm design},
 booktitle = {ICML},
 year = {2018},
 file = {Alaa, van der Schaar 2018 - Limits of estimating heterogeneous treatment:Attachments/Alaa, van der Schaar 2018 - Limits of estimating heterogeneous treatment.pdf:application/pdf;Ahmed Alaa, Mihaela Schaar 2018 - Limits of Estimating Heterogeneous Treatment:Attachments/Ahmed Alaa, Mihaela Schaar 2018 - Limits of Estimating Heterogeneous Treatment.pdf:application/pdf}
}


@inproceedings{Alaa.2019,
 abstract = {Proceedings of the International Conference on Machine Learning 2019},
 author = {Alaa, Ahmed M. and {van der Schaar}, Mihaela},
 title = {Validating causal inference models via influence functions},
 keywords = {boring formatting information;ICML;Machine learning},
 booktitle = {ICML},
 year = {2019},
 file = {alaa19a:Attachments/alaa19a.pdf:application/pdf}
}


@article{Allam.2021,
 abstract = {In digital medicine, patient data typically record health events over time (eg, through electronic health records, wearables, or other sensing technologies) and thus form unique patient trajectories. Patient trajectories are highly predictive of the future course of diseases and therefore facilitate effective care. However, digital medicine often uses only limited patient data, consisting of health events from only a single or small number of time points while ignoring additional information encoded in patient trajectories. To analyze such rich longitudinal data, new artificial intelligence (AI) solutions are needed. In this paper, we provide an overview of the recent efforts to develop trajectory-aware AI solutions and provide suggestions for future directions. Specifically, we examine the implications for developing disease models from patient trajectories along the typical workflow in AI: problem definition, data processing, modeling, evaluation, and interpretation. We conclude with a discussion of how such AI solutions will allow the field to build robust models for personalized risk scoring, subtyping, and disease pathway discovery.},
 author = {Allam, Ahmed and Feuerriegel, Stefan and Rebhan, Michael and Krauthammer, Michael},
 year = {2021},
 title = {Analyzing patient trajectories with artificial intelligence},
 pages = {e29812},
 volume = {23},
 number = {12},
 journal = {Journal of Medical Internet Research},
 doi = {10.2196/29812}
}


@article{Angrist.1990,
 author = {Angrist, Joshua D.},
 year = {1990},
 title = {Lifetime earnings and the vietnam era draft lotter: Evidence from social security administrative records},
 pages = {313--336},
 volume = {80},
 number = {3},
 journal = {The American Economic Review},
 file = {2006669:Attachments/2006669.pdf:application/pdf}
}


@article{Angrist.1991,
 author = {Angrist, Joshua D. and Krueger, Alan B.},
 year = {1991},
 title = {Does compulsory school attendance affect schooling and earnings?},
 pages = {979--1014},
 volume = {106},
 number = {4},
 journal = {The Quarterly Journal of Economics},
 file = {w3572:Attachments/w3572.pdf:application/pdf}
}


@article{Angrist.1996,
 author = {Angrist, Joshua D. and Imbens, Guido W. and Rubin, Donald B.},
 year = {1996},
 title = {Identification of causal effects using instrumental variables},
 pages = {444--455},
 volume = {91},
 number = {434},
 journal = {Journal of the American Statistical Association},
 file = {2291629 (1):Attachments/2291629 (1).pdf:application/pdf}
}


@book{Angrist.2008,
 author = {Angrist, Joshua D. and Pischke, J{\"o}rn-Steffen},
 year = {2008},
 title = {Most harmless econometrics},
 address = {Princeton},
 publisher = {{Princeton University Press}}
}


@misc{Athey.,
 author = {Athey, Susan and Wager, Stefan},
 title = {Heterogeneous Treatment Effects},
 file = {CATE{\_}tutorial{\_}short (2):Attachments/CATE{\_}tutorial{\_}short (2).pdf:application/pdf}
}


@article{Athey.2017,
 author = {Athey, Susan and Imbens, Guido and Pham, Thai and Wager, Stefan},
 year = {2017},
 title = {Estimating average treatment effects: Supplementary analyses and remaining challenges: Supplementary analyses and remaining challenges},
 pages = {278--281},
 volume = {107},
 number = {5},
 issn = {0002-8282},
 journal = {American Economic Review},
 doi = {10.1257/aer.p20171042}
}


@article{Athey.2021,
 abstract = {In many areas, practitioners seek to use observational data to learn a treatment assignment policy that satisfies application-specific constraints, such as budget, fairness, simplicity, or other functional form constraints. For example, policies may be restricted to take the form of decision trees based on a limited set of easily observable individual characteristics. We propose a new approach to this problem motivated by the theory of semiparametrically efficient estimation. Our method can be used to optimize either binary treatments or infinitesimal nudges to continuous treatments, and can leverage observational data where causal effects are identified using a variety of strategies, including selection on observables and instrumental variables. Given a doubly robust estimator of the causal effect of assigning everyone to treatment, we develop an algorithm for choosing whom to treat, and establish strong guarantees for the asymptotic utilitarian regret of the resulting policy.},
 author = {Athey, Susan and Wager, Stefan},
 year = {2021},
 title = {Policy learning with observational data},
 keywords = {Computer Science - Learning;Mathematics - Statistics;Statistics - Machine Learning;Statistics - Theory},
 pages = {133--161},
 volume = {89},
 number = {1},
 journal = {Econometrica},
 file = {1702.02896:Attachments/1702.02896.pdf:application/pdf}
}


@article{Bang.2005,
 abstract = {The goal of this article is to construct doubly robust (DR) estimators in ignorable missing data and causal inference models. In a missing data model, an estimator is DR if it remains consistent when either (but not necessarily both) a model for the missingness mechanism or a model for the distribution of the complete data is correctly specified. Because with observational data one can never be sure that either a missingness model or a complete data model is correct, perhaps the best that can be hoped for is to find a DR estimator. DR estimators, in contrast to standard likelihood-based or (nonaugmented) inverse probability-weighted estimators, give the analyst two chances, instead of only one, to make a valid inference. In a causal inference model, an estimator is DR if it remains consistent when either a model for the treatment assignment mechanism or a model for the distribution of the counterfactual data is correctly specified. Because with observational data one can never be sure that a model for the treatment assignment mechanism or a model for the counterfactual data is correct, inference based on DR estimators should improve upon previous approaches. Indeed, we present the results of simulation studies which demonstrate that the finite sample performance of DR estimators is as impressive as theory would predict. The proposed method is applied to a cardiovascular clinical trial.},
 author = {Bang, Heejung and Robins, James M.},
 year = {2005},
 title = {Doubly robust estimation in missing data and causal inference models},
 pages = {962--973},
 volume = {61},
 number = {4},
 issn = {0006-341X},
 journal = {Biometrics},
 doi = {10.1111/j.1541-0420.2005.00377.x},
 file = {Biometrics - 2005 - Bang - Doubly Robust Estimation in Missing Data and Causal Inference Models:Attachments/Biometrics - 2005 - Bang - Doubly Robust Estimation in Missing Data and Causal Inference Models.pdf:application/pdf}
}


@article{BargagliStoffi.2021,
 author = {Bargagli-Stoffi, Falco J. and de Witte, Kristof and Gnecco, Giorgio},
 year = {2021},
 title = {Heterogeneous causal eﬀects with imperfect compliance: A Bayesian machine learning approach},
 journal = {Annals of Applied Statistics},
 file = {2021-12-10 - dps 2113:Attachments/2021-12-10 - dps 2113.pdf:application/pdf}
}


@inproceedings{Bennett.2019,
 abstract = {Instrumental variable analysis is a powerful tool for estimating causal effects when randomization or full control of confounders is not possible. The application of standard methods such as 2SLS, GMM, and more recent variants are significantly impeded when the causal effects are complex, the instruments are high-dimensional, and/or the treatment is high-dimensional. In this paper, we propose the DeepGMM algorithm to overcome this. Our algorithm is based on a new variational reformulation of GMM with optimal inverse-covariance weighting that allows us to efficiently control very many moment conditions. We further develop practical techniques for optimization and model selection that make it particularly successful in practice. Our algorithm is also computationally tractable and can handle large-scale datasets. Numerical results show our algorithm matches the performance of the best tuned methods in standard settings and continues to work in high-dimensional settings where even recent methods break.},
 author = {Bennett, Andrew and Kallus, Nathan and Schnabel, Tobias},
 title = {Deep generalized method of moments for instrumental variable analysis},
 url = {http://arxiv.org/pdf/1905.12495v2},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {NeurIPS},
 year = {2019},
 file = {1905.12495:Attachments/1905.12495.pdf:application/pdf}
}


@article{Berrevoets.04.02.2022,
 abstract = {Missing data is a systemic problem in practical scenarios that causes noise and bias when estimating treatment effects. This makes treatment effect estimation from data with missingness a particularly tricky endeavour. A key reason for this is that standard assumptions on missingness are rendered insufficient due to the presence of an additional variable, treatment, besides the individual and the outcome. Having a treatment variable introduces additional complexity with respect to why some variables are missing that is not fully explored by previous work. In our work we identify a new missingness mechanism, which we term mixed confounded missingness (MCM), where some missingness determines treatment selection and other missingness is determined by treatment selection. Given MCM, we show that naively imputing all data leads to poor performing treatment effects models, as the act of imputation effectively removes information necessary to provide unbiased estimates. However, no imputation at all also leads to biased estimates, as missingness determined by treatment divides the population in distinct subpopulations, where estimates across these populations will be biased. Our solution is selective imputation, where we use insights from MCM to inform precisely which variables should be imputed and which should not. We empirically demonstrate how various learners benefit from selective imputation compared to other solutions for missing data.},
 author = {Berrevoets, Jeroen and Imrie, Fergus and Kyono, Trent and Jordon, James and {van der Schaar}, Mihaela},
 title = {To Impute or not to Impute? -- Missing Data in Treatment Effect  Estimation},
 url = {http://arxiv.org/pdf/2202.02096v1},
 journal = {arXiv preprint},
 file = {Berrevoets, Imrie et al. 04.02.2022 - To Impute or not:Attachments/Berrevoets, Imrie et al. 04.02.2022 - To Impute or not.pdf:application/pdf}
}


@misc{Berrevoets.2021,
 abstract = {Choosing the best treatment-plan for each individual patient requires accurate forecasts of their outcome trajectories as a function of the treatment, over time. While large observational data sets constitute rich sources of information to learn from, they also contain biases as treatments are rarely assigned randomly in practice. To provide accurate and unbiased forecasts, we introduce the Disentangled Counterfactual Recurrent Network (DCRN), a novel sequence-to-sequence architecture that estimates treatment outcomes over time by learning representations of patient histories that are disentangled into three separate latent factors: a treatment factor, influencing only treatment selection; an outcome factor, influencing only the outcome; and a confounding factor, influencing both. With an architecture that is completely inspired by the causal structure of treatment influence over time, we advance forecast accuracy and disease understanding, as our architecture allows for practitioners to infer which patient features influence which part in a patient's trajectory, contrasting other approaches in this domain. We demonstrate that DCRN outperforms current state-of-the-art methods in forecasting treatment responses, on both real and simulated data.},
 author = {Berrevoets, Jeroen and Curth, Alicia and Bica, Ioana and McKinney, Eoin and {van der Schaar}, Mihaela},
 date = {2021},
 title = {Disentangled counterfactual recurrent networks for treatment effect  inference over time},
 url = {http://arxiv.org/pdf/2112.03811v1},
 keywords = {Computer Science - Learning},
 file = {2112.03811:Attachments/2112.03811.pdf:application/pdf}
}


@inproceedings{Bica.2020,
 abstract = {Identifying when to give treatments to patients and how to select among multiple treatments over time are important medical problems with a few existing solutions. In this paper, we introduce the Counterfactual Recurrent Network (CRN), a novel sequence-to-sequence model that leverages the increasingly available patient observational data to estimate treatment effects over time and answer such medical questions. To handle the bias from time-varying confounders, covariates affecting the treatment assignment policy in the observational data, CRN uses domain adversarial training to build balancing representations of the patient history. At each timestep, CRN constructs a treatment invariant representation which removes the association between patient history and treatment assignments and thus can be reliably used for making counterfactual predictions. On a simulated model of tumour growth, with varying degree of time-dependent confounding, we show how our model achieves lower error in estimating counterfactuals and in choosing the correct treatment and timing of treatment than current state-of-the-art methods.},
 author = {Bica, Ioana and Alaa, Ahmed M. and Jordon, James and {van der Schaar}, Mihaela},
 title = {Estimating counterfactual treatment outcomes over time through adversarially balanced representations},
 booktitle = {ICLR},
 year = {2020}
}


@inproceedings{Bica.2020b,
 abstract = {Time Series Deconfounder: Estimating Treatment Effects over Time in the Presence of Hidden ConfoundersIoana Bica,~Ahmed Alaa,~Mihaela Van Der Schaa...},
 author = {Bica, Ioana and Alaa, Ahmed M. and {van der Schaar}, Mihaela},
 title = {Time series deconfounder: Estimating treatment effects over time in the presence of hidden confounders},
 booktitle = {ICML},
 year = {2020},
 file = {Ioana Bica, Ahmed Alaa et al. 2020 - Time Series Deconfounder:Attachments/Ioana Bica, Ahmed Alaa et al. 2020 - Time Series Deconfounder.pdf:application/pdf}
}


@inproceedings{Bica.2020c,
 abstract = {While much attention has been given to the problem of estimating the effect of discrete interventions from observational data, relatively little work has been done in the setting of continuous-valued interventions, such as treatments associated with a dosage parameter. In this paper, we tackle this problem by building on a modification of the generative adversarial networks (GANs) framework. Our model, SCIGAN, is flexible and capable of simultaneously estimating counterfactual outcomes for several different continuous interventions. The key idea is to use a significantly modified GAN model to learn to generate counterfactual outcomes, which can then be used to learn an inference model, using standard supervised methods, capable of estimating these counterfactuals for a new sample. To address the challenges presented by shifting to continuous interventions, we propose a novel architecture for our discriminator - we build a hierarchical discriminator that leverages the structure of the continuous intervention setting. Moreover, we provide theoretical results to support our use of the GAN framework and of the hierarchical discriminator. In the experiments section, we introduce a new semi-synthetic data simulation for use in the continuous intervention setting and demonstrate improvements over the existing benchmark models.},
 author = {Bica, Ioana and Jordon, James and {van der Schaar}, Mihaela},
 title = {Estimating the effects of continuous-valued interventions using generative adversarial networks},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {NeurIPS},
 year = {2020},
 file = {2002.12326:Attachments/2002.12326.pdf:application/pdf}
}


@inproceedings{Bica.2021,
 author = {Bica, Ioana and Jarrett, Daniel and {van der Schaar}, Mihaela},
 title = {Invariant causal imitation learning for generalizable policies},
 booktitle = {NeurIPS},
 year = {2021},
 file = {NeurIPS-2021-invariant-causal-imitation-learning-for-generalizable-policies-Paper:Attachments/NeurIPS-2021-invariant-causal-imitation-learning-for-generalizable-policies-Paper.pdf:application/pdf}
}


@book{Bishop.2006,
 author = {Bishop, Christopher M.},
 year = {2006},
 title = {Pattern recognition and machine learning},
 keywords = {Machine learning;Pattern perception},
 address = {New York},
 publisher = {Springer},
 isbn = {0387310738},
 series = {Information science and statistics},
 file = {Bishop - Pattern Recognition And Machine Learning - Springer  2006:Attachments/Bishop - Pattern Recognition And Machine Learning - Springer  2006.pdf:application/pdf}
}


@article{Boruvka.2018,
 abstract = {In mobile health interventions aimed at behavior change and maintenance, treatments are provided in real time to manage current or impending high risk situations or promote healthy behaviors in near real time. Currently there is great scientific interest in developing data analysis approaches to guide the development of mobile interventions. In particular data from mobile health studies might be used to examine effect moderators-individual characteristics, time-varying context or past treatment response that moderate the effect of current treatment on a subsequent response. This paper introduces a formal definition for moderated effects in terms of potential outcomes, a definition that is particularly suited to mobile interventions, where treatment occasions are numerous, individuals are not always available for treatment, and potential moderators might be influenced by past treatment. Methods for estimating moderated effects are developed and compared. The proposed approach is illustrated using BASICS-Mobile, a smartphone-based intervention designed to curb heavy drinking and smoking among college students.},
 author = {Boruvka, Audrey and Almirall, Daniel and Witkiewitz, Katie and Murphy, Susan A.},
 year = {2018},
 title = {Assessing time-varying causal effect moderation in mobile health},
 keywords = {Effect modification;mHealth;Structural nested mean model},
 pages = {1112--1121},
 volume = {113},
 number = {523},
 journal = {Journal of the American Statistical Association},
 file = {Assessing Time-Varying Causal Effect Moderation in Mobile:Attachments/Assessing Time-Varying Causal Effect Moderation in Mobile.pdf:application/pdf;Boruvka, Almirall et al. 2018 - Assessing time-varying causal effect moderation:Attachments/Boruvka, Almirall et al. 2018 - Assessing time-varying causal effect moderation.pdf:application/pdf}
}


@book{Chakraborty.2013,
 author = {Chakraborty, Bibhas and Moodie, Erica E.M.},
 year = {2013},
 title = {Statistical Methods for Dynamic Treatment Regimes},
 address = {New York, NY},
 publisher = {{Springer New York}},
 isbn = {978-1-4614-7427-2},
 doi = {10.1007/978-1-4614-7428-9},
 file = {Statistical Methods:Attachments/Statistical Methods.pdf:application/pdf}
}


@article{Chernozhukov.2013,
 abstract = {Econometrica 2013.81:2205-2268},
 author = {Chernozhukov, Victor and Fern{\'a}ndez-Val, Ivan and Melly, Blaise},
 year = {2013},
 title = {Inference on Counterfactual Distributions},
 keywords = {Counterfactual distribution;decomposition analysis;distribution regression;duration/transformation regression;exchangeable bootstrap;Hadamard differentiability of the counterfactual operator;policy analysis;quantile regression;unconditional quantile and distribution effects},
 pages = {2205--2268},
 volume = {81},
 number = {6},
 journal = {Econometrica},
 doi = {10.3982/ECTA10582},
 file = {Econometrica - 2013 - Chernozhukov - Inference on Counterfactual Distributions:Attachments/Econometrica - 2013 - Chernozhukov - Inference on Counterfactual Distributions.pdf:application/pdf}
}


@article{Chernozhukov.2018,
 author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James M.},
 year = {2018},
 title = {Double/debiased machine learning for treatment and structural parameters},
 pages = {C1-C68},
 volume = {21},
 number = {1},
 issn = {1368-4221},
 journal = {The Econometrics Journal},
 doi = {10.1111/ectj.12097},
 file = {Double-debiased machine learning for treatment and structural parameters:Attachments/Double-debiased machine learning for treatment and structural parameters.pdf:application/pdf}
}


@proceedings{CHIL.2020,
 year = {2020},
 title = {CHIL}
}


@article{Chipman.2010,
 abstract = {The Annals of Applied Statistics , 2010, Vol.4, No.1, 266-298},
 author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
 year = {2010},
 title = {{BART}: Bayesian additive regression trees},
 keywords = {Bayesian backfitting;boosting;CART;classification;ensemble;MCMC;nonparametric regression;probit model;random basis;regularizatio;sum-of-trees model;variable selection;weak learner},
 pages = {266--298},
 volume = {4},
 number = {1},
 journal = {The Annals of Applied Statistics},
 doi = {10.1214/09-AOAS285},
 file = {09-AOAS285:Attachments/09-AOAS285.pdf:application/pdf}
}


@proceedings{CIKM.2021,
 year = {2021},
 title = {CIKM}
}


@article{Cinelli.2020,
 author = {Cinelli, Carlos and Forney, Andrew and Pearl, Judea},
 year = {2020},
 title = {A Crash Course in Good and Bad Controls},
 journal = {SSRN Electronic Journal},
 doi = {10.2139/ssrn.3689437},
 file = {SSRN-id3689437:Attachments/SSRN-id3689437.pdf:application/pdf}
}


@inproceedings{Coston.2020,
 abstract = {Algorithms are commonly used to predict outcomes under a particular decision or intervention, such as predicting whether an offender will succeed on parole if placed under minimal supervision. Generally, to learn such counterfactual prediction models from observational data on historical decisions and corresponding outcomes, one must measure all factors that jointly affect the outcomes and the decision taken. Motivated by decision support applications, we study the counterfactual prediction task in the setting where all relevant factors are captured in the historical data, but it is either undesirable or impermissible to use some such factors in the prediction model. We refer to this setting as runtime confounding. We propose a doubly-robust procedure for learning counterfactual prediction models in this setting. Our theoretical analysis and experimental results suggest that our method often outperforms competing approaches. We also present a validation procedure for evaluating the performance of counterfactual prediction methods.},
 author = {Coston, Amanda and Kennedy, Edward H. and Chouldechova, Alexandra},
 title = {Counterfactual predictions under runtime confounding},
 keywords = {Computer Science - Learning;Statistics - Machine Learning;Statistics - Methodology},
 booktitle = {NeurIPS},
 year = {2020},
 file = {2006.16916:Attachments/2006.16916.pdf:application/pdf}
}


@inproceedings{Crabbe.2021,
 author = {Crabb{\'e}, Jonathan and Qian, Zhaozhi and Imrie, Fergus and {van der Schaar}, Mihaela},
 title = {Explaining latent representations with a corpus of examples},
 booktitle = {NeurIPS},
 year = {2021},
 file = {NeurIPS-2021-explaining-latent-representations-with-a-corpus-of-examples-Paper:Attachments/NeurIPS-2021-explaining-latent-representations-with-a-corpus-of-examples-Paper.pdf:application/pdf}
}


@article{Cui.2021,
 abstract = {There is a fast-growing literature on estimating optimal treatment regimes based on randomized trials or observational studies under a key identifying condition of no unmeasured confounding. Because confounding by unmeasured factors cannot generally be ruled out with certainty in observational studies or randomized trials subject to noncompliance, we propose a general instrumental variable approach to learning optimal treatment regimes under endogeneity. Specifically, we establish identification of both value function {\$}E[Y{\_}{\mathcal{D}(L)}]{\$} for a given regime {\$}\mathcal{D}{\$} and optimal regimes {\$}\text{argmax}{\_}{\mathcal{D}} E[Y{\_}{\mathcal{D}(L)}]{\$} with the aid of a binary instrumental variable, when no unmeasured confounding fails to hold. We also construct novel multiply robust classification-based estimators. Furthermore, we propose to identify and estimate optimal treatment regimes among those who would comply to the assigned treatment under a standard monotonicity assumption. In this latter case, we establish the somewhat surprising result that complier optimal regimes can be consistently estimated without directly collecting compliance information and therefore without the complier average treatment effect itself being identified. Our approach is illustrated via extensive simulation studies and a data application on the effect of child rearing on labor participation.},
 author = {Cui, Yifan and Tchetgen, Eric Tchetgen},
 year = {2021},
 title = {A semiparametric instrumental variable approach to optimal treatment  regimes under endogeneity},
 keywords = {Mathematics - Statistics;Statistics - Machine Learning;Statistics - Methodology;Statistics - Theory},
 pages = {126--137},
 volume = {116},
 number = {553},
 journal = {Journal of the American Statistical Association},
 file = {A semiparametric instrumental variable approach to optimal treatment regimes under endogeneity:Attachments/A semiparametric instrumental variable approach to optimal treatment regimes under endogeneity.pdf:application/pdf}
}


@article{Curth.2020,
 abstract = {We aim to construct a class of learning algorithms that are of practical value to applied researchers in fields such as biostatistics, epidemiology and econometrics, where the need to learn from incompletely observed information is ubiquitous. We propose a new framework for statistical machine learning of target functions arising as identifiable functionals from statistical models, which we call `IF-learning' due to its reliance on influence functions (IFs). This framework is problem- and model-agnostic and can be used to estimate a broad variety of target parameters of interest in applied statistics: we can consider any target function for which an IF of a population-averaged version exists in analytic form. Throughout, we put particular focus on so-called coarsening at random/doubly robust problems with partially unobserved information. This includes problems such as treatment effect estimation and inference in the presence of missing outcome data. Within this framework, we propose two general learning algorithms that build on the idea of nonparametric plug-in bias removal via IFs: the 'IF-learner' which uses pseudo-outcomes motivated by uncentered IFs for regression in large samples and outputs entire target functions without confidence bands, and the 'Group-IF-learner', which outputs only approximations to a function but can give confidence estimates if sufficient information on coarsening mechanisms is available. We apply both in a simulation study on inferring treatment effects.},
 author = {Curth, Alicia and Alaa, Ahmed M. and {van der Schaar}, Mihaela},
 year = {2020},
 title = {Estimating structural target functions using machine learning and  influence functions},
 url = {http://arxiv.org/pdf/2008.06461v3},
 keywords = {Statistics - Machine Learning;Statistics - Methodology},
 journal = {arXiv preprint},
 file = {2008.06461:Attachments/2008.06461.pdf:application/pdf}
}


@inproceedings{Curth.2021,
 abstract = {The need to evaluate treatment effectiveness is ubiquitous in most of empirical science, and interest in flexibly investigating effect heterogeneity is growing rapidly. To do so, a multitude of model-agnostic, nonparametric meta-learners have been proposed in recent years. Such learners decompose the treatment effect estimation problem into separate sub-problems, each solvable using standard supervised learning methods. Choosing between different meta-learners in a data-driven manner is difficult, as it requires access to counterfactual information. Therefore, with the ultimate goal of building better understanding of the conditions under which some learners can be expected to perform better than others a priori, we theoretically analyze four broad meta-learning strategies which rely on plug-in estimation and pseudo-outcome regression. We highlight how this theoretical reasoning can be used to guide principled algorithm design and translate our analyses into practice by considering a variety of neural network architectures as base-learners for the discussed meta-learning strategies. In a simulation study, we showcase the relative strengths of the learners under different data-generating processes.},
 author = {Curth, Alicia and {van der Schaar}, Mihaela},
 title = {Nonparametric estimation of heterogeneous treatment effects: From theory  to learning Algorithms},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {AISTATS},
 year = {2021},
 file = {2101.10943:Attachments/2101.10943.pdf:application/pdf}
}


@inproceedings{Curth.2021b,
 author = {Curth, Alicia and Lee, Changhee and {van der Schaar}, Mihaela},
 title = {Surv{ITE}: Learning heterogeneous treatment effects from time-to-event data},
 booktitle = {NeurIPS},
 year = {2021},
 file = {NeurIPS-2021-survite-learning-heterogeneous-treatment-effects-from-time-to-event-data-Paper:Attachments/NeurIPS-2021-survite-learning-heterogeneous-treatment-effects-from-time-to-event-data-Paper.pdf:application/pdf}
}


@inproceedings{Curth.2021c,
 author = {Curth, Alicia and {van der Schaar}, Mihaela},
 title = {On inductive biases for heterogeneous treatment effect estimation},
 booktitle = {NeurIPS},
 year = {2021},
 file = {NeurIPS-2021-on-inductive-biases-for-heterogeneous-treatment-effect-estimation-Paper:Attachments/NeurIPS-2021-on-inductive-biases-for-heterogeneous-treatment-effect-estimation-Paper.pdf:application/pdf}
}


@article{Curth.2021d,
 abstract = {The machine learning toolbox for estimation of heterogeneous treatment effects from observational data is expanding rapidly, yet many of its algorithms have been evaluated only on a very limited set of semi-synthetic benchmark datasets. In this paper, we show that even in arguably the simplest setting -- estimation under ignorability assumptions -- the results of such empirical evaluations can be misleading if (i) the assumptions underlying the data-generating mechanisms in benchmark datasets and (ii) their interplay with baseline algorithms are inadequately discussed. We consider two popular machine learning benchmark datasets for evaluation of heterogeneous treatment effect estimators -- the IHDP and ACIC2016 datasets -- in detail. We identify problems with their current use and highlight that the inherent characteristics of the benchmark datasets favor some algorithms over others -- a fact that is rarely acknowledged but of immense relevance for interpretation of empirical results. We close by discussing implications and possible next steps.},
 author = {Curth, Alicia and {van der Schaar}, Mihaela},
 title = {Doing great at estimating {CATE}? On the neglected assumptions in benchmark comparisons of treatment effect estimators},
 url = {http://arxiv.org/pdf/2107.13346v1},
 keywords = {Computer Science - Learning;ICML;Machine learning;Statistics - Methodology},
 journal = {arXiv preprint},
 file = {2107.13346:Attachments/2107.13346.pdf:application/pdf}
}


@inproceedings{Dai.2018,
 abstract = {When function approximation is used, solving the Bellman optimality equation with stability guarantees has remained a major open problem in reinforcement learning for decades. The fundamental difficulty is that the Bellman operator may become an expansion in general, resulting in oscillating and even divergent behavior of popular algorithms like Q-learning. In this paper, we revisit the Bellman equation, and reformulate it into a novel primal-dual optimization problem using Nesterov's smoothing technique and the Legendre-Fenchel transformation. We then develop a new algorithm, called Smoothed Bellman Error Embedding, to solve this optimization problem where any differentiable function class may be used. We provide what we believe to be the first convergence guarantee for general nonlinear function approximation, and analyze the algorithm's sample complexity. Empirically, our algorithm compares favorably to state-of-the-art baselines in several benchmark control problems.},
 author = {Dai, Bo and Shaw, Albert and Li, Lihong and Xiao, Lin and He, Niao and Liu, Zhen and Chen, Jianshu and Song, Le},
 title = {{SBEED}: Convergent Reinforcement Learning with Nonlinear Function  Approximation},
 keywords = {Computer Science - Learning;Markov Decision Processes;Reinforcement Learning},
 booktitle = {ICML},
 year = {2018},
 file = {dai18c:Attachments/dai18c.pdf:application/pdf}
}


@inproceedings{DAmour.2019,
 abstract = {Unobserved confounding is a central barrier to drawing causal inferences from observational data. Several authors have recently proposed that this barrier can be overcome in the case where one attempts to infer the effects of several variables simultaneously. In this paper, we present two simple, analytical counterexamples that challenge the general claims that are central to these approaches. In addition, we show that nonparametric identification is impossible in this setting. We discuss practical implications, and suggest alternatives to the methods that have been proposed so far in this line of work: using proxy variables and shifting focus to sensitivity analysis.},
 author = {D'Amour, Alexander},
 title = {On multi-cause causal inference with unobserved confounding: Counterexamples, impossibility, and alternatives},
 url = {http://arxiv.org/pdf/1902.10286v4},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {AISTATS},
 year = {2019},
 file = {1902.10286:Attachments/1902.10286.pdf:application/pdf}
}


@article{Daniel.2013,
 abstract = {Longitudinal studies, where data are repeatedly collected on subjects over a period, are common in medical research. When estimating the effect of a time-varying treatment or exposure on an outcome of interest measured at a later time, standard methods fail to give consistent estimators in the presence of time-varying confounders if those confounders are themselves affected by the treatment. Robins and colleagues have proposed several alternative methods that, provided certain assumptions hold, avoid the problems associated with standard approaches. They include the g-computation formula, inverse probability weighted estimation of marginal structural models and g-estimation of structural nested models. In this tutorial, we give a description of each of these methods, exploring the links and differences between them and the reasons for choosing one over the others in different settings.},
 author = {Daniel, Rhian. M. and Cousens, Simon N. and de Stavola, Bianca L. and Kenward, Mike G. and Sterne, Jonathan A. C.},
 year = {2013},
 title = {Methods for dealing with time-dependent confounding},
 keywords = {Data Interpretation, Statistical;Humans;Longitudinal Studies;Models, Statistical},
 pages = {1584--1618},
 volume = {32},
 number = {9},
 journal = {Statistics in Medicine},
 doi = {10.1002/sim.5686},
 file = {daniel2012:Attachments/daniel2012.pdf:application/pdf}
}


@inproceedings{Daskalakis.2018,
 abstract = {We address the issue of limit cycling behavior in training Generative Adversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for training Wasserstein GANs. Recent theoretical results have shown that optimistic mirror decent (OMD) can enjoy faster regret rates in the context of zero-sum games. WGANs is exactly a context of solving a zero-sum game with simultaneous no-regret dynamics. Moreover, we show that optimistic mirror decent addresses the limit cycling problem in training WGANs. We formally show that in the case of bi-linear zero-sum games the last iterate of OMD dynamics converges to an equilibrium, in contrast to GD dynamics which are bound to cycle. We also portray the huge qualitative difference between GD and OMD dynamics with toy examples, even when GD is modified with many adaptations proposed in the recent literature, such as gradient penalty or momentum. We apply OMD WGAN training to a bioinformatics problem of generating DNA sequences. We observe that models trained with OMD achieve consistently smaller KL divergence with respect to the true underlying distribution, than models trained with GD variants. Finally, we introduce a new algorithm, Optimistic Adam, which is an optimistic variant of Adam. We apply it to WGAN training on CIFAR10 and observe improved performance in terms of inception score as compared to Adam.},
 author = {Daskalakis, Constantinos and Ilyas, Andrew and Syrgkanis, Vasilis and Zeng, Haoyang},
 title = {Training {GAN}s with optimism},
 url = {http://arxiv.org/pdf/1711.00141v2},
 keywords = {Computer Science - Computer Science and Game Theory;Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {ICLR},
 year = {2018},
 file = {1711.00141:Attachments/1711.00141.pdf:application/pdf}
}


@article{Dippel.2019,
 author = {Dippel, Christian and Gold, Robert and Heblich, Stephan and Pinto, Rodrigo},
 year = {2019},
 title = {Mediation analysis in {IV} settings with a single instrument},
 journal = {arXiv preprint},
 file = {Mediation Analysis in IV Settings with a Single Instrument:Attachments/Mediation Analysis in IV Settings with a Single Instrument.pdf:application/pdf}
}


@article{Farbmacher.2020,
 abstract = {This paper combines causal mediation analysis with double machine learning to control for observed confounders in a data-driven way under a selection-on-observables assumption in a high-dimensional setting. We consider the average indirect effect of a binary treatment operating through an intermediate variable (or mediator) on the causal path between the treatment and the outcome, as well as the unmediated direct effect. Estimation is based on efficient score functions, which possess a multiple robustness property w.r.t. misspecifications of the outcome, mediator, and treatment models. This property is key for selecting these models by double machine learning, which is combined with data splitting to prevent overfitting in the estimation of the effects of interest. We demonstrate that the direct and indirect effect estimators are asymptotically normal and root-n consistent under specific regularity conditions and investigate the finite sample properties of the suggested methods in a simulation study when considering lasso as machine learner. We also provide an empirical application to the U.S. National Longitudinal Survey of Youth, assessing the indirect effect of health insurance coverage on general health operating via routine checkups as mediator, as well as the direct effect. We find a moderate short term effect of health insurance coverage on general health which is, however, not mediated by routine checkups.},
 author = {Farbmacher, Helmut and Huber, Martin and Laff{\'e}rs, Luk{\'a}{\v{s}} and Langen, Henrika and Spindler, Martin},
 year = {2020},
 title = {Causal mediation analysis with double machine learning},
 url = {http://arxiv.org/pdf/2002.12710v6},
 journal = {arXiv preprint},
 file = {2002.12710:Attachments/2002.12710.pdf:application/pdf}
}


@misc{Feuerriegel.2021,
 author = {Feuerriegel, Stefan},
 title = {How to write scientific papers},
 file = {LMU{\_}Managing your research process{\_}v1:Attachments/LMU{\_}Managing your research process{\_}v1.pdf:application/pdf}
}


@article{Finkelstein.2012,
 abstract = {In 2008, a group of uninsured low-income adults in Oregon was selected by lottery to be given the chance to apply for Medicaid. This lottery provides an opportunity to gauge the effects of expanding access to public health insurance on the health care use, financial strain, and health of low-income adults using a randomized controlled design. In the year after random assignment, the treatment group selected by the lottery was about 25 percentage points more likely to have insurance than the control group that was not selected. We find that in this first year, the treatment group had substantively and statistically significantly higher health care utilization (including primary and preventive care as well as hospitalizations), lower out-of-pocket medical expenditures and medical debt (including fewer bills sent to collection), and better self-reported physical and mental health than the control group.},
 author = {Finkelstein, Amy and Taubman, Sarah and Wright, Bill and Bernstein, Mira and Gruber, Jonathan and Newhouse, Joseph P. and Allen, Heidi and Baicker, Katherine},
 year = {2012},
 title = {The oregon health insurance experiment: Evidence from the first year},
 pages = {1057--1106},
 volume = {127},
 number = {3},
 journal = {The Quarterly Journal of Economics},
 doi = {10.1093/qje/qjs020},
 file = {qje{\_}qjs020:Attachments/qje{\_}qjs020.pdf:application/pdf}
}


@article{Fisher.2021,
 abstract = {Estimators based on influence functions (IFs) have been shown to be effective in many settings, especially when combined with machine learning techniques. By focusing on estimating a specific target of interest (e.g., the average effect of a treatment), rather than on estimating the full underlying data generating distribution, IF-based estimators are often able to achieve asymptotically optimal mean-squared error. Still, many researchers find IF-based estimators to be opaque or overly technical, which makes their use less prevalent and their benefits less available. To help foster understanding and trust in IF-based estimators, we present tangible, visual illustrations of when and how IF-based estimators can outperform standard ``plug-in'' estimators. The figures we show are based on connections between IFs, gradients, linear approximations, and Newton-Raphson.},
 author = {Fisher, Aaron and Kennedy, Edward H.},
 year = {2021},
 title = {Visually communicating and teaching Intuition for influence functions},
 keywords = {Mathematics - Statistics;Statistics - Methodology;Statistics - Theory},
 pages = {162--172},
 volume = {75},
 number = {2},
 journal = {The American Statistician},
 file = {1810.03260:Attachments/1810.03260.pdf:application/pdf}
}


@article{Foster.2019,
 abstract = {We provide non-asymptotic excess risk guarantees for statistical learning in a setting where the population risk with respect to which we evaluate the target parameter depends on an unknown nuisance parameter that must be estimated from data. We analyze a two-stage sample splitting meta-algorithm that takes as input two arbitrary estimation algorithms: one for the target parameter and one for the nuisance parameter. We show that if the population risk satisfies a condition called Neyman orthogonality, the impact of the nuisance estimation error on the excess risk bound achieved by the meta-algorithm is of second order. Our theorem is agnostic to the particular algorithms used for the target and nuisance and only makes an assumption on their individual performance. This enables the use of a plethora of existing results from statistical learning and machine learning to give new guarantees for learning with a nuisance component. Moreover, by focusing on excess risk rather than parameter estimation, we can give guarantees under weaker assumptions than in previous works and accommodate settings in which the target parameter belongs to a complex nonparametric class. We provide conditions on the metric entropy of the nuisance and target classes such that oracle rates---rates of the same order as if we knew the nuisance parameter---are achieved. We also derive new rates for specific estimation algorithms such as variance-penalized empirical risk minimization, neural network estimation and sparse high-dimensional linear model estimation. We highlight the applicability of our results in four settings of central importance: 1) heterogeneous treatment effect estimation, 2) offline policy optimization, 3) domain adaptation, and 4) learning with missing data.},
 author = {Foster, Dylan J. and Syrgkanis, Vasilis},
 year = {2019},
 title = {Orthogonal statistical learning},
 url = {http://arxiv.org/pdf/1901.09036v3},
 file = {Foster, Syrgkanis 25.01.2019 - Orthogonal Statistical Learning:Attachments/Foster, Syrgkanis 25.01.2019 - Orthogonal Statistical Learning.pdf:application/pdf}
}


@misc{Frauen.02.03.2022,
 abstract = {In medical practice, treatments are selected based on the expected causal effects on patient outcomes. Here, the gold standard for estimating causal effects are randomized controlled trials; however, such trials are costly and sometimes even unethical. Instead, medical practice is increasingly interested in estimating causal effects among patient subgroups from electronic health records, that is, observational data. In this paper, we aim at estimating the average causal effect (ACE) from observational data (patient trajectories) that are collected over time. For this, we propose DeepACE: an end-to-end deep learning model. DeepACE leverages the iterative G-computation formula to adjust for the bias induced by time-varying confounders. Moreover, we develop a novel sequential targeting procedure which ensures that DeepACE has favorable theoretical properties, i.e., is doubly robust and asymptotically efficient. To the best of our knowledge, this is the first work that proposes an end-to-end deep learning model for estimating time-varying ACEs. We compare DeepACE in an extensive number of experiments, confirming that it achieves state-of-the-art performance. We further provide a case study for patients suffering from low back pain to demonstrate that DeepACE generates important and meaningful findings for clinical practice. Our work enables medical practitioners to develop effective treatment recommendations tailored to patient subgroups.},
 author = {Frauen, Dennis and Hatt, Tobias and Melnychuk, Valentyn and Feuerriegel, Stefan},
 date = {02.03.2022},
 title = {Estimating average causal effects from patient trajectories},
 url = {http://arxiv.org/pdf/2203.01228v1},
 file = {Frauen, Hatt et al. 02.03.2022 - Estimating average causal effects:Attachments/Frauen, Hatt et al. 02.03.2022 - Estimating average causal effects.pdf:application/pdf}
}


@article{Frölich.2017,
 abstract = {IZA Discussion Paper No. 8280},
 author = {Fr{\"o}lich, Markus and Huber, Martin},
 year = {2017},
 title = {Direct and indirect treatment effects--causal chains and mediation analysis with instrumental variables},
 keywords = {direct effect;indirect effect;instrument;treatment effects},
 pages = {1645--1666},
 volume = {79},
 number = {5},
 issn = {1467-9868},
 journal = {Journal of the Royal Statistical Society: Series B},
 doi = {10.1920/wp.cem.2014.3114},
 file = {Direct and Indirect Treatment Effects - Causal Chains and Mediation Analysis with Instrumental Variables:Attachments/Direct and Indirect Treatment Effects - Causal Chains and Mediation Analysis with Instrumental Variables.pdf:application/pdf}
}


@inproceedings{Gal.2016,
 abstract = {Neural Information Processing Systems http://nips.cc/},
 author = {Gal, Yarin and Ghahramani, Zoubin},
 title = {A theoretically grounded application of dropout in recurrent neural networks},
 booktitle = {NeurIPS},
 year = {2016},
 file = {NIPS-2016-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks-Paper:Attachments/NIPS-2016-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks-Paper.pdf:application/pdf}
}


@inproceedings{Ganin.2015,
 abstract = {Proceedings of the International Conference on Machine Learning 2015},
 author = {Ganin, Yaroslav and Lempitsky, Victor},
 title = {Unsupervised domain adaptation by backpropagation},
 keywords = {Deep Learning;Gradient Reversal;Unsupervised Domain Adaptation},
 booktitle = {ICML},
 year = {2015},
 file = {ganin15:Attachments/ganin15.pdf:application/pdf}
}


@article{Glass.2013,
 abstract = {Causal inference has a central role in public health; the determination that an association is causal indicates the possibility for intervention. We review and comment on the long-used guidelines for interpreting evidence as supporting a causal association and contrast them with the potential outcomes framework that encourages thinking in terms of causes that are interventions. We argue that in public health this framework is more suitable, providing an estimate of an action's consequences rather than the less precise notion of a risk factor's causal effect. A variety of modern statistical methods adopt this approach. When an intervention cannot be specified, causal relations can still exist, but how to intervene to change the outcome will be unclear. In application, the often-complex structure of causal processes needs to be acknowledged and appropriate data collected to study them. These newer approaches need to be brought to bear on the increasingly complex public health challenges of our globalized world.},
 author = {Glass, Thomas A. and Goodman, Steven N. and Hern{\'a}n, Miguel A. and Samet, Jonathan M.},
 year = {2013},
 title = {Causal inference in public health},
 pages = {61--75},
 volume = {34},
 journal = {Annual Review of Public Health}
}


@article{Gueyffier.1997,
 abstract = {BACKGROUND

Trials of drug therapy for hypertension have shown that such therapy has a clear overall benefit in preventing cardiovascular disease. Although these trials have included slightly more women than men, it is still not clear whether treatment benefit is similar for both sexes.

OBJECTIVE

To quantify the average treatment effect in both sexes and to determine whether available data show significant differences in treatment effect between women and men.

DESIGN

Subgroup meta-analysis of individual patient data according to sex. Analysis was based on seven trials from the INDANA (INdividual Data ANalysis of Antihypertensive intervention trials) database and was adjusted for possible confounders.

PATIENTS

20,802 women and 19,975 men recruited between 1972 and 1990.

INTERVENTIONS

Primarily beta-blockers and thiazide diuretics.

RESULTS

In women, treatment effect was statistically significant for stroke (fatal strokes and all strokes) and for major cardiovascular events. In men, it was statistically significant for all categories of events (total and specific mortality, all coronary events, all strokes, and major cardiovascular events). The odds ratios for any category of event did not differ significantly between men and women. In absolute terms, the benefit in women was seen primarily for strokes; in men, treatment prevented as many coronary events as strokes. Graphical analyses suggest that these results could be completely explained by the difference in untreated risk.

CONCLUSIONS

In terms of relative risk, treatment benefit did not differ between women and men. The absolute risk reduction attributable to treatment seemed to depend on untreated risk. These findings underline the need to predict accurately the untreated cardiovascular risk of an individual person in order to rationalize and individualize antihypertensive treatment.},
 author = {Gueyffier, Francois and Boutitie, Florent and Boissel, Jean P. and Pocock, Stuart and Coope, John and Cutler, Jeffrey and Ekbom, Tord and Fagard, Robert and Friedman, Lawrence and Perry, Mitchell and Prineas, Ronald and Schron, Eleanor},
 year = {1997},
 title = {Effect of antihypertensive drug treatment on cardiovascular outcomes in women and men. A meta-analysis of individual patient data from randomized, controlled trials. The INDANA Investigators},
 pages = {761--767},
 volume = {126},
 number = {10},
 issn = {0003-4819},
 journal = {Annals of Internal Medicine},
 doi = {10.7326/0003-4819-126-10-199705150-00002}
}


@inproceedings{Gutmann.2010,
 author = {Gutmann, Michael and Hyv{\"a}rinen, Aapo},
 title = {Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
 booktitle = {AISTATS},
 year = {2010},
 file = {gutmann10a:Attachments/gutmann10a.pdf:application/pdf}
}


@article{Hamburg.2010,
 author = {Hamburg, Margaret A. and Collins, Francis S.},
 year = {2010},
 title = {The path to personalized medicine},
 pages = {301--304},
 volume = {363},
 number = {4},
 journal = {The New England Journal of Medicine},
 doi = {10.1056/NEJMp1006304}
}


@inproceedings{Hartford.2017,
 abstract = {Deep IV: A Flexible Approach for Counterfactual PredictionJason Hartford,~Greg Lewis,~Kevin Leyton-Brown,~Matt TaddyCounterfactual prediction ...},
 author = {Hartford, Jason and Lewis, Greg and Leyton-Brown, Kevin and Taddy, Matt},
 title = {Deep {IV}: A flexible approach for counterfactual prediction},
 booktitle = {ICML},
 year = {2017},
 file = {Jason Hartford, Greg Lewis et al. 2017 - Deep IV A Flexible Approach:Attachments/Jason Hartford, Greg Lewis et al. 2017 - Deep IV A Flexible Approach.pdf:application/pdf}
}


@inproceedings{Hatt.2021,
 abstract = {Decision-making often requires accurate estimation of treatment effects from observational data. This is challenging as outcomes of alternative decisions are not observed and have to be estimated. Previous methods estimate outcomes based on unconfoundedness but neglect any constraints that unconfoundedness imposes on the outcomes. In this paper, we propose a novel regularization framework for estimating average treatment effects that exploits unconfoundedness. To this end, we formalize unconfoundedness as an orthogonality constraint, which ensures that the outcomes are orthogonal to the treatment assignment. This orthogonality constraint is then included in the loss function via a regularization. Based on our regularization framework, we develop deep orthogonal networks for unconfounded treatments (DONUT), which learn outcomes that are orthogonal to the treatment assignment. Using a variety of benchmark datasets for estimating average treatment effects, we demonstrate that DONUT outperforms the state-of-the-art substantially.},
 author = {Hatt, Tobias and Feuerriegel, Stefan},
 title = {Estimating average treatment effects via orthogonal regularization},
 booktitle = {CIKM},
 year = {2021},
 file = {Hatt, Feuerriegel 21 01 2021 - Estimating Average Treatment Effects:Attachments/Hatt, Feuerriegel 21 01 2021 - Estimating Average Treatment Effects.pdf:application/pdf}
}


@article{Hatt.2021b,
 abstract = {Using observational data to estimate the effect of a treatment is a powerful tool for decision-making when randomized experiments are infeasible or costly. However, observational data often yields biased estimates of treatment effects, since treatment assignment can be confounded by unobserved variables. A remedy is offered by deconfounding methods that adjust for such unobserved confounders. In this paper, we develop the Sequential Deconfounder, a method that enables estimating individualized treatment effects over time in presence of unobserved confounders. This is the first deconfounding method that can be used in a general sequential setting (i.e., with one or more treatments assigned at each timestep). The Sequential Deconfounder uses a novel Gaussian process latent variable model to infer substitutes for the unobserved confounders, which are then used in conjunction with an outcome model to estimate treatment effects over time. We prove that using our method yields unbiased estimates of individualized treatment responses over time. Using simulated and real medical data, we demonstrate the efficacy of our method in deconfounding the estimation of treatment responses over time.},
 author = {Hatt, Tobias and Feuerriegel, Stefan},
 year = {2021},
 title = {Sequential deconfounding for causal inference with unobserved  confounders},
 url = {http://arxiv.org/pdf/2104.09323v2},
 journal = {arXiv preprint}
}


@article{Hatt.2021c,
 abstract = {Learning personalized decision policies that generalize to the target population is of great relevance. Since training data is often not representative of the target population, standard policy learning methods may yield policies that do not generalize target population. To address this challenge, we propose a novel framework for learning policies that generalize to the target population. For this, we characterize the difference between the training data and the target population as a sample selection bias using a selection variable. Over an uncertainty set around this selection variable, we optimize the minimax value of a policy to achieve the best worst-case policy value on the target population. In order to solve the minimax problem, we derive an efficient algorithm based on a convex-concave procedure and prove convergence for parametrized spaces of policies such as logistic policies. We prove that, if the uncertainty set is well-specified, our policies generalize to the target population as they can not do worse than on the training data. Using simulated data and a clinical trial, we demonstrate that, compared to standard policy learning methods, our framework improves the generalizability of policies substantially.},
 author = {Hatt, Tobias and Tschernutter, Daniel and Feuerriegel, Stefan},
 year = {2021},
 title = {Generalizing off-policy learning under sample selection bias},
 url = {http://arxiv.org/pdf/2112.01387v1},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 journal = {arXiv preprint},
 file = {2112.01387:Attachments/2112.01387.pdf:application/pdf}
}


@article{Hatt.2022,
 abstract = {Estimating heterogeneous treatment effects is an important problem across many domains. In order to accurately estimate such treatment effects, one typically relies on data from observational studies or randomized experiments. Currently, most existing works rely exclusively on observational data, which is often confounded and, hence, yields biased estimates. While observational data is confounded, randomized data is unconfounded, but its sample size is usually too small to learn heterogeneous treatment effects. In this paper, we propose to estimate heterogeneous treatment effects by combining large amounts of observational data and small amounts of randomized data via representation learning. In particular, we introduce a two-step framework: first, we use observational data to learn a shared structure (in form of a representation); and then, we use randomized data to learn the data-specific structures. We analyze the finite sample properties of our framework and compare them to several natural baselines. As such, we derive conditions for when combining observational and randomized data is beneficial, and for when it is not. Based on this, we introduce a sample-efficient algorithm, called CorNet. We use extensive simulation studies to verify the theoretical properties of CorNet and multiple real-world datasets to demonstrate our method's superiority compared to existing methods.},
 author = {Hatt, Tobias and Berrevoets, Jeroen and Curth, Alicia and Feuerriegel, Stefan and {van der Schaar}, Mihaela},
 year = {2022},
 title = {Combining observational and randomized data for estimating heterogeneous treatment effects},
 url = {http://arxiv.org/pdf/2202.12891v1},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 journal = {arXiv preprint},
 file = {2202.12891 (1):Attachments/2202.12891 (1).pdf:application/pdf}
}


@article{Hochreiter.1997,
 abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
 author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
 year = {1997},
 title = {Long short-term memory},
 pages = {1735--1780},
 volume = {9},
 number = {8},
 issn = {0899-7667},
 journal = {Neural Computation},
 doi = {10.1162/neco.1997.9.8.1735}
}


@article{Huang.2020,
 author = {Huang, Biwei and Zhang, Kun and Zhang, Jiji and Ramsey, Joseph and Sanchez-Romero, Ruben and Glymour, Clark and Sch{\"o}lkopf, Bernhard},
 year = {2020},
 title = {Causal discovery from heterogeneous/nonstationary data},
 volume = {21},
 journal = {Journal of Machine Learning Research},
 file = {19-232:Attachments/19-232.pdf:application/pdf}
}


@article{Hudgens.2008,
 abstract = {A fundamental assumption usually made in causal inference is that of no interference between individuals (or units); that is, the potential outcomes of one individual are assumed to be unaffected by the treatment assignment of other individuals. However, in many settings, this assumption obviously does not hold. For example, in the dependent happenings of infectious diseases, whether one person becomes infected depends on who else in the population is vaccinated. In this article, we consider a population of groups of individuals where interference is possible between individuals within the same group. We propose estimands for direct, indirect, total, and overall causal effects of treatment strategies in this setting. Relations among the estimands are established; for example, the total causal effect is shown to equal the sum of direct and indirect causal effects. Using an experimental design with a two-stage randomization procedure (first at the group level, then at the individual level within groups), unbiased estimators of the proposed estimands are presented. Variances of the estimators are also developed. The methodology is illustrated in two different settings where interference is likely: assessing causal effects of housing vouchers and of vaccines.},
 author = {Hudgens, Michael G. and Halloran, M. Elizabeth},
 year = {2008},
 title = {Toward causal inference with interference},
 pages = {832--842},
 volume = {103},
 number = {482},
 journal = {Journal of the American Statistical Association},
 doi = {10.1198/016214508000000292},
 file = {nihms-73860:Attachments/nihms-73860.pdf:application/pdf}
}


@proceedings{ICDM.2020,
 year = {2020},
 title = {ICDM},
 doi = {10.1109/ICDM50108.2020}
}


@proceedings{ICLR.2015,
 year = {2015},
 title = {ICLR}
}


@proceedings{ICLR.2018,
 year = {2018},
 title = {ICLR}
}


@proceedings{ICLR.2020,
 year = {2020},
 title = {ICLR}
}


@proceedings{ICLR.2021,
 year = {2021},
 title = {ICLR}
}


@proceedings{ICML.2015,
 year = {2015},
 title = {ICML}
}


@proceedings{ICML.2016,
 year = {2016},
 title = {ICML}
}


@proceedings{ICML.2017,
 year = {2017},
 title = {ICML}
}


@proceedings{ICML.2018,
 year = {2018},
 title = {ICML}
}


@proceedings{ICML.2019,
 year = {2019},
 title = {ICML}
}


@proceedings{ICML.2020,
 year = {2020},
 title = {ICML}
}


@proceedings{ICML.2021,
 year = {2021},
 title = {ICML}
}


@article{Imbens.1994,
 author = {Imbens, Guido W. and Angrist, Joshua D.},
 year = {1994},
 title = {Identification and estimation of local average treatment effects},
 pages = {467--475},
 volume = {62},
 number = {2},
 journal = {Econometrica},
 file = {2951620 (1):Attachments/2951620 (1).pdf:application/pdf}
}


@article{Imbens.2022,
 abstract = {We study the identification and estimation of long-term treatment effects when both experimental and observational data are available. Since the long-term outcome is observed only after a long delay, it is not measured in the experimental data, but only recorded in the observational data. However, both types of data include observations of some short-term outcomes. In this paper, we uniquely tackle the challenge of persistent unmeasured confounders, i.e., some unmeasured confounders that can simultaneously affect the treatment, short-term outcomes and the long-term outcome, noting that they invalidate identification strategies in previous literature. To address this challenge, we exploit the sequential structure of multiple short-term outcomes, and develop three novel identification strategies for the average long-term treatment effect. We further propose three corresponding estimators and prove their asymptotic consistency and asymptotic normality. We finally apply our methods to estimate the effect of a job training program on long-term employment using semi-synthetic data. We numerically show that our proposals outperform existing methods that fail to handle persistent confounders.},
 author = {Imbens, Guido and Kallus, Nathan and Mao, Xiaojie and Wang, Yuhao},
 year = {2022},
 title = {Long-term Causal Inference Under Persistent Confounding via Data  Combination},
 url = {http://arxiv.org/pdf/2202.07234v1},
 keywords = {Statistics - Machine Learning;Statistics - Methodology},
 journal = {arXiv preprint},
 file = {2202.07234:Attachments/2202.07234.pdf:application/pdf}
}


@article{Jerolon.2020,
 abstract = {Mediation analysis aims at disentangling the effects of a treatment on an outcome through alternative causal mechanisms and has become a popular practice in biomedical and social science applications. The causal framework based on counterfactuals is currently the standard approach to mediation, with important methodological advances introduced in the literature in the last decade, especially for simple mediation, that is with one mediator at the time. Among a variety of alternative approaches, Imai et al. showed theoretical results and developed an R package to deal with simple mediation as well as with multiple mediation involving multiple mediators conditionally independent given the treatment and baseline covariates. This approach does not allow to consider the often encountered situation in which an unobserved common cause induces a spurious correlation between the mediators. In this context, which we refer to as mediation with uncausally related mediators, we show that, under appropriate hypothesis, the natural direct and joint indirect effects are non-parametrically identifiable. Moreover, we adopt the quasi-Bayesian algorithm developed by Imai et al. and propose a procedure based on the simulation of counterfactual distributions to estimate not only the direct and joint indirect effects but also the indirect effects through individual mediators. We study the properties of the proposed estimators through simulations. As an illustration, we apply our method on a real data set from a large cohort to assess the effect of hormone replacement treatment on breast cancer risk through three mediators, namely dense mammographic area, nondense area and body mass index.},
 author = {J{\'e}rolon, Allan and Baglietto, Laura and Birmel{\'e}, Etienne and Alarcon, Flora and Perduca, Vittorio},
 year = {2020},
 title = {Causal mediation analysis in presence of multiple mediators uncausally related},
 keywords = {Bayes Theorem;Causality;Cohort Studies;correlated mediators;direct and indirect effects;Humans;independent mediators;Mediation Analysis;Models, Statistical;multiple mediators;simulation of counterfactuals},
 pages = {191--221},
 volume = {17},
 number = {2},
 journal = {The International Journal of Biostatistics},
 doi = {10.1515/ijb-2019-0088},
 file = {10.1515{\_}ijb-2019-0088:Attachments/10.1515{\_}ijb-2019-0088.pdf:application/pdf}
}


@inproceedings{Jesson.2021,
 author = {Jesson, Andrew and Mindermann, S{\"o}ren and Gal, Yarin and Shalit, Uri},
 title = {Quantifying ignorance in individual-level causal-effect estimates under hidden confounding},
 booktitle = {ICML},
 year = {2021},
 file = {jesson21a:Attachments/jesson21a.pdf:application/pdf}
}


@inproceedings{Johansson.2016,
 abstract = {Proceedings of the International Conference on Machine Learning 2016},
 author = {Johansson, Fredrik D. and Shalit, Uri and Sonntag, David},
 title = {Learning representations for counterfactual inference},
 keywords = {causal inference;domain adaptation;representation learning},
 booktitle = {ICML},
 year = {2016},
 file = {Learning Representations for Counterfactual Inference:Attachments/Learning Representations for Counterfactual Inference.pdf:application/pdf}
}


@article{Johnson.2016,
 abstract = {MIMIC-III ('Medical Information Mart for Intensive Care') is a large, single-center database comprising information relating to patients admitted to critical care units at a large tertiary care hospital. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. The database supports applications including academic and industrial research, quality improvement initiatives, and higher education coursework.},
 author = {Johnson, Alistair E. W. and Pollard, Tom J. and Shen, Lu and Lehman, Li-wei H. and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Celi, Leo Anthony and Mark, Roger G.},
 year = {2016},
 title = {{MIMIC-III}, a freely accessible critical care database},
 pages = {160035},
 volume = {3},
 number = {1},
 issn = {2052-4463},
 journal = {Scientific Data},
 doi = {10.1038/sdata.2016.35},
 file = {Johnson, Pollard et al. 2016 - MIMIC-III, a freely accessible critical:Attachments/Johnson, Pollard et al. 2016 - MIMIC-III, a freely accessible critical.pdf:application/pdf}
}


@inproceedings{Kallus.2018,
 abstract = {We present a new approach to the problems of evaluating and learning personalized decision policies from observational data of past contexts, decisions, and outcomes. Only the outcome of the enacted decision is available and the historical policy is unknown. These problems arise in personalized medicine using electronic health records and in internet advertising. Existing approaches use inverse propensity weighting (or, doubly robust versions) to make historical outcome (or, residual) data look like it were generated by a new policy being evaluated or learned. But this relies on a plug-in approach that rejects data points with a decision that disagrees with the new policy, leading to high variance estimates and ineffective learning. We propose a new, balance-based approach that too makes the data look like the new policy but does so directly by finding weights that optimize for balance between the weighted data and the target policy in the given, finite sample, which is equivalent to minimizing worst-case or posterior conditional mean square error. Our policy learner proceeds as a two-level optimization problem over policies and weights. We demonstrate that this approach markedly outperforms existing ones both in evaluation and learning, which is unsurprising given the wider support of balance-based weights. We establish extensive theoretical consistency guarantees and regret bounds that support this empirical success.},
 author = {Kallus, Nathan},
 title = {Balanced policy evaluation and learning},
 booktitle = {NeurIPS},
 year = {2018},
 file = {Kallus 2018 - Balanced Policy Evaluation and Learning:Attachments/Kallus 2018 - Balanced Policy Evaluation and Learning.pdf:application/pdf}
}


@inproceedings{Kallus.2018b,
 abstract = {Valid causal inference in observational studies often requires controlling for confounders. However, in practice measurements of confounders may be noisy, and can lead to biased estimates of causal effects. We show that we can reduce the bias caused by measurement noise using a large number of noisy measurements of the underlying confounders. We propose the use of matrix factorization to infer the confounders from noisy covariates, a flexible and principled framework that adapts to missing values, accommodates a wide variety of data types, and can augment many causal inference methods. We bound the error for the induced average treatment effect estimator and show it is consistent in a linear regression setting, using Exponential Family Matrix Completion preprocessing. We demonstrate the effectiveness of the proposed procedure in numerical experiments with both synthetic data and real clinical data.},
 author = {Kallus, Nathan and Mao, Xiaojie and Udell, Madeleine},
 title = {Causal inference with noisy and missing covariates via matrix factorization},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {NeurIPS},
 year = {2018},
 file = {1806.00811v1:Attachments/1806.00811v1.pdf:application/pdf}
}


@article{Kennedy.2016,
 abstract = {In this paper we review important aspects of semiparametric theory and empirical processes that arise in causal inference problems. We begin with a brief introduction to the general problem of causal inference, and go on to discuss estimation and inference for causal effects under semiparametric models, which allow parts of the data-generating process to be unrestricted if they are not of particular interest (i.e., nuisance functions). These models are very useful in causal problems because the outcome process is often complex and difficult to model, and there may only be information available about the treatment process (at best). Semiparametric theory gives a framework for benchmarking efficiency and constructing estimators in such settings. In the second part of the paper we discuss empirical process theory, which provides powerful tools for understanding the asymptotic behavior of semiparametric estimators that depend on flexible nonparametric estimators of nuisance functions. These tools are crucial for incorporating machine learning and other modern methods into causal inference analyses. We conclude by examining related extensions and future directions for work in semiparametric causal inference.},
 author = {Kennedy, Edward H.},
 year = {2016},
 title = {Semiparametric theory and empirical processes in causal inference},
 keywords = {Mathematics - Statistics;Statistics - Theory},
 pages = {141--167},
 journal = {Statistical Causal Inferences and their Applications in Public Health Research},
 file = {1510.04740:Attachments/1510.04740.pdf:application/pdf}
}


@misc{Kennedy.2018,
 author = {Kennedy, Edward H.},
 title = {Nonparametric efficiency theory and machine learning in causal inference},
 file = {tutorial:Attachments/tutorial.pdf:application/pdf}
}


@article{Kennedy.2020,
 abstract = {Heterogeneous effect estimation plays a crucial role in causal inference, with applications across medicine and social science. Many methods for estimating conditional average treatment effects (CATEs) have been proposed in recent years, but there are important theoretical gaps in understanding if and when such methods are optimal. This is especially true when the CATE has nontrivial structure (e.g., smoothness or sparsity). Our work contributes in several main ways. First, we study a two-stage doubly robust CATE estimator and give a generic model-free error bound, which, despite its generality, yields sharper results than those in the current literature. We apply the bound to derive error rates in nonparametric models with smoothness or sparsity, and give sufficient conditions for oracle efficiency. Underlying our error bound is a general oracle inequality for regression with estimated or imputed outcomes, which is of independent interest; this is the second main contribution. The third contribution is aimed at understanding the fundamental statistical limits of CATE estimation. To that end, we propose and study a local polynomial adaptation of double-residual regression. We show that this estimator can be oracle efficient under even weaker conditions, if used with a specialized form of sample splitting and careful choices of tuning parameters. These are the weakest conditions currently found in the literature, and we conjecture that they are minimal in a minimax sense. We go on to give error bounds in the non-trivial regime where oracle rates cannot be achieved. Some finite-sample properties are explored with simulations.},
 author = {Kennedy, Edward H.},
 year = {2020},
 title = {Optimal doubly robust estimation of heterogeneous causal effects},
 url = {http://arxiv.org/pdf/2004.14497v2},
 keywords = {Mathematics - Statistics;Statistics - Theory},
 journal = {arXiv preprint},
 file = {2004.14497:Attachments/2004.14497.pdf:application/pdf}
}


@article{Kennedy.2022,
 abstract = {In this review we cover the basics of efficient nonparametric parameter estimation (also called functional estimation), with a focus on parameters that arise in causal inference problems. We review both efficiency bounds (i.e., what is the best possible performance for estimating a given parameter?) and the analysis of particular estimators (i.e., what is this estimator's error, and does it attain the efficiency bound?) under weak assumptions. We emphasize minimax-style efficiency bounds, worked examples, and practical shortcuts for easing derivations. We gloss over most technical details, in the interest of highlighting important concepts and providing intuition for main ideas.},
 author = {Kennedy, Edward H.},
 year = {2022},
 title = {Semiparametric doubly robust targeted double machine learning: A review},
 url = {http://arxiv.org/pdf/2203.06469v1},
 keywords = {Statistics - Methodology},
 journal = {arXiv preprint},
 file = {2203.06469:Attachments/2203.06469.pdf:application/pdf}
}


@article{Kennedy.2022b,
 abstract = {Estimation of heterogeneous causal effects - i.e., how effects of policies and treatments vary across subjects - is a fundamental task in causal inference, playing a crucial role in optimal treatment allocation, generalizability, subgroup effects, and more. Many flexible methods for estimating conditional average treatment effects (CATEs) have been proposed in recent years, but questions surrounding optimality have remained largely unanswered. In particular, a minimax theory of optimality has yet to be developed, with the minimax rate of convergence and construction of rate-optimal estimators remaining open problems. In this paper we derive the minimax rate for CATE estimation, in a nonparametric model where distributional components are Holder-smooth, and present a new local polynomial estimator, giving high-level conditions under which it is minimax optimal. More specifically, our minimax lower bound is derived via a localized version of the method of fuzzy hypotheses, combining lower bound constructions for nonparametric regression and functional estimation. Our proposed estimator can be viewed as a local polynomial R-Learner, based on a localized modification of higher-order influence function methods; it is shown to be minimax optimal under a condition on how accurately the covariate distribution is estimated. The minimax rate we find exhibits several interesting features, including a non-standard elbow phenomenon and an unusual interpolation between nonparametric regression and functional estimation rates. The latter quantifies how the CATE, as an estimand, can be viewed as a regression/functional hybrid. We conclude with some discussion of a few remaining open problems.},
 author = {Kennedy, Edward H. and Balakrishnan, Sivaraman and Wasserman, Larry},
 year = {2022},
 title = {Minimax rates for heterogeneous causal effect estimation},
 url = {http://arxiv.org/pdf/2203.00837v1},
 keywords = {Mathematics - Statistics;Statistics - Theory},
 journal = {arXiv preprint},
 file = {2203.00837:Attachments/2203.00837.pdf:application/pdf}
}


@misc{Kennedy.24.02.2021,
 abstract = {Causal effects are often characterized with averages, which can give an incomplete picture of the underlying counterfactual distributions. Here we consider estimating the entire counterfactual density and generic functionals thereof. We focus on two kinds of target parameters. The first is a density approximation, defined by a projection onto a finite-dimensional model using a generalized distance metric, which includes f-divergences as well as $L_p$ norms. The second is the distance between counterfactual densities, which can be used as a more nuanced effect measure than the mean difference, and as a tool for model selection. We study nonparametric efficiency bounds for these targets, giving results for smooth but otherwise generic models and distances. Importantly, we show how these bounds connect to means of particular non-trivial functions of counterfactuals, linking the problems of density and mean estimation. We go on to propose doubly robust-style estimators for the density approximations and distances, and study their rates of convergence, showing they can be optimally efficient in large nonparametric models. We also give analogous methods for model selection and aggregation, when many models may be available and of interest. Our results all hold for generic models and distances, but throughout we highlight what happens for particular choices, such as $L_2$ projections on linear models, and KL projections on exponential families. Finally we illustrate by estimating the density of CD4 count among patients with HIV, had all been treated with combination therapy versus zidovudine alone, as well as a density effect. Our results suggest combination therapy may have increased CD4 count most for high-risk patients. Our methods are implemented in the freely available R package npcausal on GitHub.},
 author = {Kennedy, Edward H. and Balakrishnan, Sivaraman and Wasserman, Larry},
 date = {24.02.2021},
 title = {Semiparametric counterfactual density estimation},
 url = {http://arxiv.org/pdf/2102.12034v1},
 keywords = {Mathematics - Statistics;Statistics - Methodology;Statistics - Theory},
 file = {2102.12034:Attachments/2102.12034.pdf:application/pdf}
}


@misc{Kim.08.06.2018,
 abstract = {In this paper we develop a framework for characterizing causal effects via distributional distances. In particular we define a causal effect in terms of the $L_1$ distance between different counterfactual outcome distributions, rather than the typical mean difference in outcome values. Comparing entire counterfactual outcome distributions can provide more nuanced and valuable measures for exploring causal effects beyond the average treatment effect. First, we propose a novel way to estimate counterfactual outcome densities, which is of independent interest. Then we develop an efficient estimator of our target causal effect. We go on to provide error bounds and asymptotic properties of the proposed estimator, along with bootstrap-based confidence intervals. Finally, we illustrate the methods via simulations and real data.},
 author = {Kim, Kwangho and Kim, Jisu and Kennedy, Edward H.},
 date = {08.06.2018},
 title = {Causal effects based on distributional distances},
 url = {http://arxiv.org/pdf/1806.02935v2},
 keywords = {Computer Science - Learning;Statistics - Machine Learning;Statistics - Methodology},
 file = {1806.02935:Attachments/1806.02935.pdf:application/pdf}
}


@inproceedings{Kingma.2015,
 abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
 author = {Kingma, Diederik P. and Ba, Jimmy},
 title = {Adam: A method for stochastic optimization},
 booktitle = {ICLR},
 year = {2015}
}


@article{Kunzel.2019,
 abstract = {There is growing interest in estimating and analyzing heterogeneous treatment effects in experimental and observational studies. We describe a number of metaalgorithms that can take advantage of any supervised learning or regression method in machine learning and statistics to estimate the conditional average treatment effect (CATE) function. Metaalgorithms build on base algorithms-such as random forests (RFs), Bayesian additive regression trees (BARTs), or neural networks-to estimate the CATE, a function that the base algorithms are not designed to estimate directly. We introduce a metaalgorithm, the X-learner, that is provably efficient when the number of units in one treatment group is much larger than in the other and can exploit structural properties of the CATE function. For example, if the CATE function is linear and the response functions in treatment and control are Lipschitz-continuous, the X-learner can still achieve the parametric rate under regularity conditions. We then introduce versions of the X-learner that use RF and BART as base learners. In extensive simulation studies, the X-learner performs favorably, although none of the metalearners is uniformly the best. In two persuasion field experiments from political science, we demonstrate how our X-learner can be used to target treatment regimes and to shed light on underlying mechanisms. A software package is provided that implements our methods.},
 author = {K{\"u}nzel, S{\"o}ren R. and Sekhon, Jasjeet S. and Bickel, Peter J. and Yu, Bin},
 year = {2019},
 title = {Metalearners for estimating heterogeneous treatment effects using machine learning},
 pages = {4156--4165},
 volume = {116},
 number = {10},
 journal = {Proceedings of the National Academy of Sciences (PNAS)},
 doi = {10.1073/pnas.1804597116},
 file = {pnas.1804597116:Attachments/pnas.1804597116.pdf:application/pdf}
}


@inproceedings{Kuzmanovic.2021,
 author = {Kuzmanovic, Milan and Hatt, Tobias and Feuerriegel, Stefan},
 title = {Deconfounding temporal autoencoder: Estimating treatment effects over time using noisy proxies},
 booktitle = {ML4H},
 year = {2021},
 file = {kuzmanovic21a:Attachments/kuzmanovic21a.pdf:application/pdf}
}


@unpublished{Kuzmanovic.2022,
 author = {Kuzmanovic, Milan and Hatt, Tobias and Feuerriegel, Stefan},
 year = {2022},
 title = {Estimating heterogeneous treatment effects with missing treatment information},
 file = {main:Attachments/main.pdf:application/pdf}
}


@article{Kyono.11022021,
 abstract = {Selecting causal inference models for estimating individualized treatment effects (ITE) from observational data presents a unique challenge since the counterfactual outcomes are never observed. The problem is challenged further in the unsupervised domain adaptation (UDA) setting where we only have access to labeled samples in the source domain, but desire selecting a model that achieves good performance on a target domain for which only unlabeled samples are available. Existing techniques for UDA model selection are designed for the predictive setting. These methods examine discriminative density ratios between the input covariates in the source and target domain and do not factor in the model's predictions in the target domain. Because of this, two models with identical performance on the source domain would receive the same risk score by existing methods, but in reality, have significantly different performance in the test domain. We leverage the invariance of causal structures across domains to propose a novel model selection metric specifically designed for ITE methods under the UDA setting. In particular, we propose selecting models whose predictions of interventions' effects satisfy known causal structures in the target domain. Experimentally, our method selects ITE models that are more robust to covariate shifts on several healthcare datasets, including estimating the effect of ventilation in COVID-19 patients from different geographic locations.},
 author = {Kyono, Trent and Bica, Ioana and Qian, Zhaozhi and {van der Schaar}, Mihaela},
 title = {Selecting treatment effects models for domain adaptation using causal  knowledge},
 url = {http://arxiv.org/pdf/2102.06271v1},
 keywords = {Computer Science - Learning;ICML;Machine learning},
 journal = {arXiv preprint},
 file = {2102.06271:Attachments/2102.06271.pdf:application/pdf}
}


@inproceedings{Kyono.2021,
 author = {Kyono, Trent and Zhang, Yao and Bellot, Alexis and {van der Schaar}, Mihaela},
 title = {{MIRACLE}: Causally aware imputation via learning missing data mechanisms},
 booktitle = {NeurIPS},
 year = {2021},
 file = {NeurIPS-2021-miracle-causally-aware-imputation-via-learning-missing-data-mechanisms-Paper:Attachments/NeurIPS-2021-miracle-causally-aware-imputation-via-learning-missing-data-mechanisms-Paper.pdf:application/pdf}
}


@inproceedings{Li.2021,
 abstract = {G-Net: a Recurrent Network Approach to G-Computation for Counterfactual Prediction Under a Dynamic Treatment RegimeRui Li,~Stephanie Hu,~Mingyu Lu,...},
 author = {Li, Rui and Hu, Stephanie and Lu, Mingyu and Utsumi, Yuria and Chakraborty, Prithwish and Sow, Daby M. and Madan, Piyush and Li, Jun and Ghalwash, Mohamed and Shahn, Zach and Lehman, Li-wei},
 title = {G-Net: A recurrent network approach to G-computation for counterfactual prediction under a dynamic treatment regime},
 booktitle = {ML4H},
 year = {2021},
 file = {Rui Li, Stephanie Hu et al. 2021 - G-Net a Recurrent Network Approach:Attachments/Rui Li, Stephanie Hu et al. 2021 - G-Net a Recurrent Network Approach.pdf:application/pdf}
}


@article{Li.2022,
 author = {Li, Chunxiao and Rudin, Cynthia and McCormick, Typer H.},
 year = {2022},
 title = {Rethinking nonlinear instrumental variable models through prediction validity},
 pages = {1--55},
 volume = {23},
 journal = {Journal of Machine Learning Research},
 file = {21-0082:Attachments/21-0082.pdf:application/pdf}
}


@article{Liao.2020,
 abstract = {Due to the recent advancements in wearables and sensing technology, health scientists are increasingly developing mobile health (mHealth) interventions. In mHealth interventions, mobile devices are used to deliver treatment to individuals as they go about their daily lives. These treatments are generally designed to impact a near time, proximal outcome such as stress or physical activity. The mHealth intervention policies, often called just-in-time adaptive interventions, are decision rules that map an individual's current state (e.g., individual's past behaviors as well as current observations of time, location, social activity, stress and urges to smoke) to a particular treatment at each of many time points. The vast majority of current mHealth interventions deploy expert-derived policies. In this paper, we provide an approach for conducting inference about the performance of one or more such policies using historical data collected under a possibly different policy. Our measure of performance is the average of proximal outcomes over a long time period should the particular mHealth policy be followed. We provide an estimator as well as confidence intervals. This work is motivated by HeartSteps, an mHealth physical activity intervention.},
 author = {Liao, Peng and Klasnja, Predrag and Murphy, Susan},
 year = {2020},
 title = {Off-policy estimation of long-term average outcomes with applications to mobile health},
 keywords = {Computer Science - Learning;Mathematics - Statistics;Statistics - Machine Learning;Statistics - Theory},
 pages = {1--23},
 volume = {116},
 journal = {Journal of the American Statistical Association},
 file = {1912.13088:Attachments/1912.13088.pdf:application/pdf}
}


@inproceedings{Lim.2018,
 author = {Lim, Bryan and Alaa, Ahmed M. and {van der Schaar}, Mihaela},
 title = {Forecasting treatment responses over time using recurrent marginal structural networks},
 booktitle = {NeurIPS},
 year = {2018},
 file = {Forecasting Treatment Responses Over Time 2018:Attachments/Forecasting Treatment Responses Over Time 2018.pdf:application/pdf}
}


@inproceedings{Liu.2020,
 author = {Liu, Ruoqi and Yin, Changchang and Zhang, Ping},
 title = {Estimating individual treatment effects with time-varying confounders},
 booktitle = {ICDM},
 year = {2020}
}


@inproceedings{Ma.2021,
 author = {Ma, Ymupu and Tresp, Volker},
 title = {Causal inference under networked interference and intervention policy enhancement},
 booktitle = {AISTATS},
 year = {2021},
 file = {2002.08506:Attachments/2002.08506.pdf:application/pdf;ma21c:Attachments/ma21c.pdf:application/pdf}
}


@misc{MarieDavidian.,
 author = {{Marie Davidian}},
 title = {Double robustness in estimation of causal treatment effects},
 file = {Double Robustness in Estimation:Attachments/Double Robustness in Estimation.pdf:application/pdf}
}


@article{McGrath.2020,
 abstract = {Researchers are often interested in estimating the causal effects of sustained treatment strategies, i.e., of (hypothetical) interventions involving time-varying treatments. When using observational data, estimating those effects requires adjustment for confounding. However, conventional regression methods cannot appropriately adjust for confounding in the presence of treatment-confounder feedback. In contrast, estimators derived from Robins's g-formula may correctly adjust for confounding even if treatment-confounder feedback exists. The package gfoRmula implements in R one such estimator: the parametric g-formula. This estimator can be used to estimate the effects of binary or continuous time-varying treatments as well as contrasts defined by static or dynamic, deterministic, or random interventions, as well as interventions that depend on the natural value of treatment. The package accommodates survival outcomes as well as binary or continuous outcomes measured at the end of follow-up. This paper describes the gfoRmula package, along with motivating background, features, and examples.},
 author = {McGrath, Sean and Lin, Victoria and Zhang, Zilu and Petito, Lucia C. and Logan, Roger W. and Hern{\'a}n, Miguel A. and Young, Jessica G.},
 year = {2020},
 title = {gfoRmula: An R package for estimating the effects of sustained treatment strategies via the parametric G-formula},
 volume = {1},
 number = {3},
 journal = {Patterns},
 doi = {10.1016/j.patter.2020.100008}
}


@proceedings{ML4H.2016,
 year = {2016},
 title = {ML4H}
}


@proceedings{ML4H.2021,
 year = {2021},
 title = {ML4H}
}


@article{MolgaardNielsen.2020,
 abstract = {OBJECTIVES

A conceptually oriented preprocessing of a large number of potential prognostic factors may improve the development of a prognostic model. This study investigated whether various forms of conceptually oriented preprocessing or the preselection of established factors was superior to using all factors as input.

STUDY DESIGN AND SETTING

We made use of an existing project that developed two conceptually oriented subgroupings of low back pain patients. Based on the prediction of six outcome variables by seven statistical methods, this type of preprocessing was compared with medical experts' preselection of established factors, as well as using all 112 available baseline factors.

RESULTS

Subgrouping of patients was associated with low prognostic capacity. Applying a Lasso-based variable selection to all factors or to domain-specific principal component scores performed best. The preselection of established factors showed a good compromise between model complexity and prognostic capacity.

CONCLUSION

The prognostic capacity is hard to improve by means of a conceptually oriented preprocessing when compared to purely statistical approaches. However, a careful selection of already established factors combined in a simple linear model should be considered as an option when constructing a new prognostic rule based on a large number of potential prognostic factors.},
 author = {{Molgaard Nielsen}, Anne and Binding, Adrian and Ahlbrandt-Rains, Casey and Boeker, Martin and Feuerriegel, Stefan and Vach, Werner},
 year = {2020},
 title = {Exploring conceptual preprocessing for developing prognostic models: A case study in low back pain patients},
 pages = {27--34},
 volume = {122},
 journal = {Journal of Clinical Epidemiology},
 doi = {10.1016/j.jclinepi.2020.02.005}
}


@article{Muandet.2021,
 author = {Muandet, Krikamol and Kanagawa, Montonobu and Saengkyongam, Sorawit and Marukatat, Sanparith},
 year = {2021},
 title = {Counterfactual mean embeddings},
 pages = {1--71},
 volume = {22},
 journal = {Journal of Machine Learning Research},
 file = {20-185:Attachments/20-185.pdf:application/pdf}
}


@article{Murphy.2003,
 abstract = {Summary. A dynamic treatment regime is a list of decision rules, one per time interval, for how the level of treatment will be tailored through time to an individual's changing status. The goal of ...},
 author = {Murphy, Susan A.},
 year = {2003},
 title = {Optimal dynamic treatment regimes},
 pages = {331--355},
 volume = {65},
 number = {2},
 issn = {1467-9868},
 journal = {Journal of the Royal Statistical Society: Series B},
 doi = {10.1111/1467-9868.00389},
 file = {Murphy 2003 - Optimal dynamic treatment regimes:Attachments/Murphy 2003 - Optimal dynamic treatment regimes.pdf:application/pdf}
}


@article{Naimi.2017,
 abstract = {Robins' generalized methods (g methods) provide consistent estimates of contrasts (e.g. differences, ratios) of potential outcomes under a less restrictive set of identification conditions than do standard regression methods (e.g. linear, logistic, Cox regression). Uptake of g methods by epidemiologists has been hampered by limitations in understanding both conceptual and technical details. We present a simple worked example that illustrates basic concepts, while minimizing technical complications.},
 author = {Naimi, Ashley I. and Cole, Stephen R. and Kennedy, Edward H.},
 year = {2017},
 title = {An introduction to g methods},
 pages = {756--762},
 volume = {46},
 number = {2},
 journal = {International Journal of Epidemiology},
 doi = {10.1093/ije/dyw323}
}


@proceedings{NeurIPS.2016,
 year = {2016},
 title = {NeurIPS}
}


@proceedings{NeurIPS.2017,
 year = {2017},
 title = {NeurIPS}
}


@proceedings{NeurIPS.2018,
 year = {2018},
 title = {NeurIPS}
}


@proceedings{NeurIPS.2019,
 year = {2019},
 title = {NeurIPS}
}


@proceedings{NeurIPS.2020,
 year = {2020},
 title = {NeurIPS}
}


@proceedings{NeurIPS.2021,
 year = {2021},
 title = {NeurIPS}
}


@article{Newey.2003,
 author = {Newey, Whitney K. and Powell, James L.},
 year = {2003},
 title = {Instrumental variable estimation of nonparametric models},
 pages = {1565--1578},
 volume = {71},
 number = {5},
 journal = {Econometrica},
 file = {npiv:Attachments/npiv.pdf:application/pdf}
}


@article{Nie.2021,
 abstract = {Flexible estimation of heterogeneous treatment effects lies at the heart of many statistical challenges, such as personalized medicine and optimal resource allocation. In this paper, we develop a general class of two-step algorithms for heterogeneous treatment effect estimation in observational studies. We first estimate marginal effects and treatment propensities in order to form an objective function that isolates the causal component of the signal. Then, we optimize this data-adaptive objective function. Our approach has several advantages over existing methods. From a practical perspective, our method is flexible and easy to use: In both steps, we can use any loss-minimization method, e.g., penalized regression, deep neural networks, or boosting; moreover, these methods can be fine-tuned by cross validation. Meanwhile, in the case of penalized kernel regression, we show that our method has a quasi-oracle property: Even if the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same error bounds as an oracle who has a priori knowledge of these two nuisance components. We implement variants of our approach based on penalized regression, kernel ridge regression, and boosting in a variety of simulation setups, and find promising performance relative to existing baselines.},
 author = {Nie, Xinkun and Wager, Stefan},
 year = {2021},
 title = {Quasi-oracle estimation of heterogeneous treatment effects},
 keywords = {Mathematics - Statistics;Statistics - Machine Learning;Statistics - Theory},
 pages = {299--319},
 volume = {108},
 number = {2},
 issn = {0006-3444},
 journal = {Biometrika},
 file = {1712.04912:Attachments/1712.04912.pdf:application/pdf}
}


@article{Nielsen.2017,
 abstract = {BACKGROUND

Heterogeneity in patients with low back pain (LBP) is well recognised and different approaches to subgrouping have been proposed. Latent Class Analysis (LCA) is a statistical technique that is increasingly being used to identify subgroups based on patient characteristics. However, as LBP is a complex multi-domain condition, the optimal approach when using LCA is unknown. Therefore, this paper describes the exploration of two approaches to LCA that may help improve the identification of clinically relevant and interpretable LBP subgroups.

METHODS

From 928 LBP patients consulting a chiropractor, baseline data were used as input to the statistical subgrouping. In a single-stage LCA, all variables were modelled simultaneously to identify patient subgroups. In a two-stage LCA, we used the latent class membership from our previously published LCA within each of six domains of health (activity, contextual factors, pain, participation, physical impairment and psychology) (first stage) as the variables entered into the second stage of the two-stage LCA to identify patient subgroups. The description of the results of the single-stage and two-stage LCA was based on a combination of statistical performance measures, qualitative evaluation of clinical interpretability (face validity) and a subgroup membership comparison.

RESULTS

For the single-stage LCA, a model solution with seven patient subgroups was preferred, and for the two-stage LCA, a nine patient subgroup model. Both approaches identified similar, but not identical, patient subgroups characterised by (i) mild intermittent LBP, (ii) recent severe LBP and activity limitations, (iii) very recent severe LBP with both activity and participation limitations, (iv) work-related LBP, (v) LBP and several negative consequences and (vi) LBP with nerve root involvement.

CONCLUSIONS

Both approaches identified clinically interpretable patient subgroups. The potential importance of these subgroups needs to be investigated by exploring whether they can be identified in other cohorts and by examining their possible association with patient outcomes. This may inform the selection of a preferred LCA approach.},
 author = {Nielsen, Anne Molgaard and Kent, Peter and Hestbaek, Lise and Vach, Werner and Kongsted, Alice},
 year = {2017},
 title = {Identifying subgroups of patients using latent class analysis: Should we use a single-stage or a two-stage approach? A methodological study using a cohort of patients with low back pain},
 pages = {57},
 volume = {18},
 number = {1},
 journal = {BMC Musculoskeletal Disorders},
 doi = {10.1186/s12891-017-1411-x},
 file = {Nielsen, Kent et al. 2017 - Identifying subgroups of patients using:Attachments/Nielsen, Kent et al. 2017 - Identifying subgroups of patients using.pdf:application/pdf}
}


@article{Ogburn.2019,
 abstract = {(This comment has been updated to respond to Wang and Blei's rejoinder [arXiv:1910.07320].)  The premise of the deconfounder method proposed in {\textquotedbl}Blessings of Multiple Causes{\textquotedbl} by Wang and Blei [arXiv:1805.06826], namely that a variable that renders multiple causes conditionally independent also controls for unmeasured multi-cause confounding, is incorrect. This can be seen by noting that no fact about the observed data alone can be informative about ignorability, since ignorability is compatible with any observed data distribution. Methods to control for unmeasured confounding may be valid with additional assumptions in specific settings, but they cannot, in general, provide a checkable approach to causal inference, and they do not, in general, require weaker assumptions than the assumptions that are commonly used for causal inference. While this is outside the scope of this comment, we note that much recent work on applying ideas from latent variable modeling to causal inference problems suffers from similar issues.},
 author = {Ogburn, Elizabeth L. and Shpitser, Ilya and Tchetgen, Eric J. Tchetgen},
 year = {2019},
 title = {Comment on {\textquotedbl}Blessings of multiple causes{\textquotedbl}},
 url = {http://arxiv.org/pdf/1910.05438v3},
 keywords = {Statistics - Machine Learning;Statistics - Methodology},
 journal = {arXiv preprint},
 file = {1910.05438 (1):Attachments/1910.05438 (1).pdf:application/pdf}
}


@article{Ogburn.2020,
 abstract = {We describe semiparametric estimation and inference for causal effects using observational data from a single social network. Our asymptotic result is the first to allow for dependence of each observation on a growing number of other units as sample size increases. While previous methods have generally implicitly focused on one of two possible sources of dependence among social network observations, we allow for both dependence due to transmission of information across network ties, and for dependence due to latent similarities among nodes sharing ties. We describe estimation and inference for new causal effects that are specifically of interest in social network settings, such as interventions on network ties and network structure. Using our methods to reanalyze the Framingham Heart Study data used in one of the most influential and controversial causal analyses of social network data, we find that after accounting for network structure there is no evidence for the causal effects claimed in the original paper.},
 author = {Ogburn, Elizabeth L. and Sofrygin, Oleg and Diaz, Ivan and {van der Laan}, Mark J.},
 year = {2020},
 title = {Causal inference for social network data},
 url = {http://arxiv.org/pdf/1705.08527v5},
 keywords = {Mathematics - Statistics;Statistics - Methodology;Statistics - Theory},
 journal = {arXiv preprint},
 file = {1705.08527:Attachments/1705.08527.pdf:application/pdf}
}


@book{Pearl.2009,
 abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. The book will open the way for including causal analysis in the standard curricula of statistics, artificial intelligence, business, epidemiology, social sciences, and economics. Students in these fields will find natural models, simple inferential procedures, and precise mathematical definitions of causal concepts that traditional texts have evaded or made unduly complicated. The first edition of Causality has led to a paradigmatic change in the way that causality is treated in statistics, philosophy, computer science, social science, and economics. Cited in more than 5,000 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interests to students and professionals in a wide variety of fields. Anyone who wishes to elucidate meaningful relationships from data, predict effects of actions and policies, assess explanations of reported events, or form theories of causal understanding and causal speech will find this book stimulating and invaluable.},
 author = {Pearl, Judea},
 year = {2009},
 title = {Causality},
 address = {New York City},
 publisher = {{Cambridge University Press}},
 isbn = {9780521895606},
 file = {Pearl 2009 - Causality:Attachments/Pearl 2009 - Causality.pdf:application/pdf}
}


@article{Pearl.2009b,
 author = {Pearl, Judea},
 year = {2009},
 title = {Causal inference in statistics: An overview},
 pages = {96--146},
 volume = {3},
 journal = {Statistics Surveys},
 doi = {10.1214/09-SS057},
 file = {09-SS057:Attachments/09-SS057.pdf:application/pdf}
}


@article{Prentice.1989,
 abstract = {Statistics in Medicine 1989.8:431-440},
 author = {Prentice, Ross L.},
 year = {1989},
 title = {Surrogate endpoints in clinical trials: Definition and operational criteria},
 pages = {431--440},
 volume = {8},
 journal = {Statistics in Medicine},
 file = {SURROGATE ENDPOINTS IN CLINICAL TRIALS:Attachments/SURROGATE ENDPOINTS IN CLINICAL TRIALS.pdf:application/pdf}
}


@inproceedings{Qian.2021,
 author = {Qian, Zhaozhi and Zhang, Yao and Bica, Ioana and Wood, Angela M. and {van der Schaar}, Mihaela},
 title = {SyncTwin: Treatment effect estimation with longitudinal outcomes},
 booktitle = {NeurIPS},
 year = {2021},
 file = {SyncTwin Treatment Effect Estimation 2021:Attachments/SyncTwin Treatment Effect Estimation 2021.pdf:application/pdf}
}


@inproceedings{Qian.2021b,
 author = {Qian, Zhaozhi and Curth, Alicia and {van der Schaar}, Mihaela},
 title = {Estimating multi-cause treatment effects via single cause perturbation},
 booktitle = {NeurIPS},
 year = {2021},
 file = {NeurIPS-2021-estimating-multi-cause-treatment-effects-via-single-cause-perturbation-Paper:Attachments/NeurIPS-2021-estimating-multi-cause-treatment-effects-via-single-cause-perturbation-Paper.pdf:application/pdf}
}


@book{Rasmussen.2008,
 author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
 year = {2008},
 title = {Gaussian processes for machine learning},
 address = {Cambridge, Mass.},
 edition = {3. print},
 publisher = {{MIT Press}},
 isbn = {9780262182539},
 series = {Adaptive computation and machine learning},
 file = {GP:Attachments/GP.pdf:application/pdf}
}


@article{Robins.1986,
 author = {Robins, James M.},
 year = {1986},
 title = {A new approach to causal inference in mortality studies with a sustained exposure period: Application to control of the healthy worker survivor effect},
 pages = {1393--1512},
 volume = {7},
 journal = {Mathematical Modelling},
 file = {1-s2.0-0270025586900886-main:Attachments/1-s2.0-0270025586900886-main.pdf:application/pdf}
}


@article{Robins.1994,
 author = {Robins, James M.},
 year = {1994},
 title = {Correcting for non-compliance in randomized trials using structural nested mean models},
 pages = {2379--2412},
 volume = {23},
 number = {8},
 issn = {0361-0926},
 journal = {Communications in Statistics - Theory and Methods},
 doi = {10.1080/03610929408831393},
 file = {Correcting for non compliance in randomized trials using structural nested mean models:Attachments/Correcting for non compliance in randomized trials using structural nested mean models.pdf:application/pdf}
}


@article{Robins.1999,
 author = {Robins, James M.},
 year = {1999},
 title = {Robust estimation in sequentially ignorable missing data and causal inference models},
 pages = {6--10},
 journal = {Proceedings of the American Statistical Association on Bayesian Statistical Science},
 file = {jsaprocpat1:Attachments/jsaprocpat1.pdf:application/pdf}
}


@article{Robins.2000,
 abstract = {In observational studies with exposures or treatments that vary over time, standard approaches for adjustment of confounding are biased when there exist time-dependent confounders that are also affected by previous treatment. This paper introduces marginal structural models, a new class of causal models that allow for improved adjustment of confounding in those situations. The parameters of a marginal structural model can be consistently estimated using a new class of estimators, the inverse-probability-of-treatment weighted estimators.},
 author = {Robins, James M. and Hern{\'a}n, Miguel A. and Brumback, Babette},
 year = {2000},
 title = {Marginal structural models and causal inference in epidemiology},
 keywords = {Anti-HIV Agents/therapeutic use;Causality;Confounding Factors, Epidemiologic;Epidemiologic Methods;HIV Infections/drug therapy/mortality;Humans;Models, Statistical;Risk Factors;Time Factors;Zidovudine/therapeutic use},
 pages = {550--560},
 volume = {11},
 number = {5},
 journal = {Epidemiology},
 doi = {10.1097/00001648-200009000-00011},
 file = {Marginal{\_}Structural{\_}Models{\_}and{\_}Causal{\_}Inference{\_}in.11:Attachments/Marginal{\_}Structural{\_}Models{\_}and{\_}Causal{\_}Inference{\_}in.11.pdf:application/pdf}
}


@book{Robins.2009,
 author = {Robins, James M. and Hern{\'a}n, Miguel A.},
 year = {2009},
 title = {Estimation of the causal effects of time-varying exposures},
 keywords = {Longitudinal method;Multivariate analysis;Regression analysis},
 address = {Boca Raton},
 publisher = {{CRC Press}},
 isbn = {9781584886587},
 series = {Chapman {\&} Hall/CRC handbooks of modern statistical methods},
 file = {abc:Attachments/abc.pdf:application/pdf}
}


@article{Rosenbaum.1983,
 author = {Rosenbaum, Paul R. and Rubin, Donald B.},
 year = {1983},
 title = {The central role of the propensity score in observational studies for causal effects},
 pages = {41--55},
 volume = {70},
 number = {1},
 issn = {0006-3444},
 journal = {Biometrika},
 doi = {10.1093/biomet/70.1.41}
}


@article{Rubin.1974,
 author = {Rubin, Donald B.},
 year = {1974},
 title = {Estimating causal effects of treatments in randomized and nonrandomized studies},
 pages = {688--701},
 volume = {66},
 number = {5},
 issn = {0022-0663},
 journal = {Journal of Educational Psychology},
 doi = {10.1037/h0037350}
}


@article{Rubin.1978,
 author = {Rubin, Donald B.},
 year = {1978},
 title = {Bayesian inference for causal effects: The role of randomization},
 keywords = {Potential outcomes},
 pages = {34--58},
 volume = {6},
 number = {1},
 issn = {0090-5364},
 journal = {The Annals of Statistics},
 doi = {10.1214/aos/1176344064},
 file = {1176344064:Attachments/1176344064.pdf:application/pdf}
}


@phdthesis{Rydgaard.,
 author = {Rydgaard, Helene},
 title = {Targeted causal learning for longitudinal data},
 file = {thesis-master:Attachments/thesis-master.pdf:application/pdf}
}


@inproceedings{Schulam.2017,
 author = {Schulam, Peter and Saria, Suchi},
 title = {Reliable decision support using counterfactual models},
 booktitle = {NeurIPS},
 year = {2017},
 file = {Schulam, Saria 2017 - Reliable Decision Support using Counterfactual (2):Attachments/Schulam, Saria 2017 - Reliable Decision Support using Counterfactual (2).pdf:application/pdf}
}


@inproceedings{Shalit.2017,
 abstract = {There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a {\textquotedbl}balanced{\textquotedbl} representation such that the induced treated and control distributions look similar. We give a novel, simple and intuitive generalization-error bound showing that the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.},
 author = {Shalit, Uri and Johansson, Fredrik D. and Sontag, David},
 title = {Estimating individual treatment effect: Generalization bounds and  algorithms},
 keywords = {causal effects;Computer Science - Artificial Intelligence;Computer Science - Learning;counterfactual inference;Statistics - Machine Learning},
 booktitle = {ICML},
 year = {2017},
 file = {Individual{\_}treatment{\_}generalization{\_}bounds:Attachments/Individual{\_}treatment{\_}generalization{\_}bounds.pdf:application/pdf}
}


@inproceedings{Shi.2019,
 abstract = {Neural Information Processing Systems http://nips.cc/},
 author = {Shi, Claudia and Blei, David M. and Veitch, Victor},
 title = {Adapting neural networks for the estimation of treatment effects},
 booktitle = {NeurIPS},
 year = {2019},
 file = {NeurIPS-2019-adapting-neural-networks-for-the-estimation-of-treatment-effects-Paper:Attachments/NeurIPS-2019-adapting-neural-networks-for-the-estimation-of-treatment-effects-Paper.pdf:application/pdf}
}


@inproceedings{Shpister.2007,
 abstract = {UAI-07 Conference},
 author = {Shpitser, Ilya and Pearl, Judea},
 title = {What counterfactuals can be tested},
 booktitle = {UAI},
 year = {2007},
 file = {1206.5294:Attachments/1206.5294.pdf:application/pdf}
}


@inproceedings{Singh.2019,
 author = {Singh, Rahul and Sahani, Maneesh and Gretton, Arthur},
 title = {Kernel instrumental variable regression},
 booktitle = {NeurIPS},
 year = {2019},
 file = {main (1):Attachments/main (1).pdf:application/pdf;kiv{\_}appendix:Attachments/kiv{\_}appendix.pdf:application/pdf}
}


@inproceedings{Soleimani.2017,
 abstract = {Treatment effects can be estimated from observational data as the difference in potential outcomes. In this paper, we address the challenge of estimating the potential outcome when treatment-dose levels can vary continuously over time. Further, the outcome variable may not be measured at a regular frequency. Our proposed solution represents the treatment response curves using linear time-invariant dynamical systems---this provides a flexible means for modeling response over time to highly variable dose curves. Moreover, for multivariate data, the proposed method: uncovers shared structure in treatment response and the baseline across multiple markers; and, flexibly models challenging correlation structure both across and within signals over time. For this, we build upon the framework of multiple-output Gaussian Processes. On simulated and a challenging clinical dataset, we show significant gains in accuracy over state-of-the-art models.},
 author = {Soleimani, Hossein and Subbaswamy, Adarsh and Saria, Suchi},
 title = {Treatment-response models for counterfactual reasoning with  continuous-time, continuous-valued interventions},
 booktitle = {UAI},
 year = {2017},
 file = {Soleimani, Subbaswamy et al. 07.04.2017 - Treatment-Response Models for Counterfactual Reasoning:Attachments/Soleimani, Subbaswamy et al. 07.04.2017 - Treatment-Response Models for Counterfactual Reasoning.pdf:application/pdf}
}


@article{Stone.1980,
 author = {Stone, Charles J.},
 year = {1980},
 title = {Optimal rates of convergence for nonparametric estimators},
 volume = {8},
 number = {6},
 issn = {0090-5364},
 journal = {Annals of Statistics},
 doi = {10.1214/aos/1176345206}
}


@inproceedings{Syrgkanis.2019,
 abstract = {We consider the estimation of heterogeneous treatment effects with arbitrary machine learning methods in the presence of unobserved confounders with the aid of a valid instrument. Such settings arise in A/B tests with an intent-to-treat structure, where the experimenter randomizes over which user will receive a recommendation to take an action, and we are interested in the effect of the downstream action. We develop a statistical learning approach to the estimation of heterogeneous effects, reducing the problem to the minimization of an appropriate loss function that depends on a set of auxiliary models (each corresponding to a separate prediction task). The reduction enables the use of all recent algorithmic advances (e.g. neural nets, forests). We show that the estimated effect model is robust to estimation errors in the auxiliary models, by showing that the loss satisfies a Neyman orthogonality criterion. Our approach can be used to estimate projections of the true effect model on simpler hypothesis spaces. When these spaces are parametric, then the parameter estimates are asymptotically normal, which enables construction of confidence sets. We applied our method to estimate the effect of membership on downstream webpage engagement on TripAdvisor, using as an instrument an intent-to-treat A/B test among 4 million TripAdvisor users, where some users received an easier membership sign-up process. We also validate our method on synthetic data and on public datasets for the effects of schooling on income.},
 author = {Syrgkanis, Vasilis and Lei, Victor and Oprescu, Miruna and Hei, Maggie and Battocchi, Keith and Lewis, Greg},
 title = {Machine learning estimation of heterogeneous treatment effects with instruments},
 url = {http://arxiv.org/pdf/1905.10176v3},
 keywords = {Computer Science - Learning;Statistics - Applications;Statistics - Machine Learning},
 booktitle = {NeurIPS},
 year = {2019},
 file = {1905.10176:Attachments/1905.10176.pdf:application/pdf}
}


@article{T.Bluemlein.,
 author = {{T. Bluemlein} and {Joel Persson} and {and Stefan Feuerriegel}},
 title = {Learning Optimal Dynamic Treatment Regimes Using Causal Tree Methods with Application to Medicine},
 journal = {arXiv preprint},
 file = {DTR:Attachments/DTR.pdf:application/pdf}
}


@article{Tchetgen.2012,
 abstract = {Whilst estimation of the marginal (total) causal effect of a point exposure on an outcome is arguably the most common objective of experimental and observational studies in the health and social sciences, in recent years, investigators have also become increasingly interested in mediation analysis. Specifically, upon evaluating the total effect of the exposure, investigators routinely wish to make inferences about the direct or indirect pathways of the effect of the exposure not through or through a mediator variable that occurs subsequently to the exposure and prior to the outcome. Although powerful semiparametric methodologies have been developed to analyze observational studies, that produce double robust and highly efficient estimates of the marginal total causal effect, similar methods for mediation analysis are currently lacking. Thus, this paper develops a general semiparametric framework for obtaining inferences about so-called marginal natural direct and indirect causal effects, while appropriately accounting for a large number of pre-exposure confounding factors for the exposure and the mediator variables. Our analytic framework is particularly appealing, because it gives new insights on issues of efficiency and robustness in the context of mediation analysis. In particular, we propose new multiply robust locally efficient estimators of the marginal natural indirect and direct causal effects, and develop a novel double robust sensitivity analysis framework for the assumption of ignorability of the mediator variable.},
 author = {Tchetgen, Eric J. Tchetgen and Shpitser, Ilya},
 year = {2012},
 title = {Semiparametric Theory for Causal Mediation Analysis: efficiency bounds, multiple robustness, and sensitivity analysis},
 pages = {1816--1845},
 volume = {40},
 number = {3},
 issn = {0090-5364},
 journal = {Annals of Statistics},
 doi = {10.1214/12-AOS990},
 file = {nihms746449:Attachments/nihms746449.pdf:application/pdf}
}


@article{TchetgenTchetgen.2012,
 abstract = {Interference is said to be present when the exposure or treatment received by one individual may affect the outcomes of other individuals. Such interference can arise in settings in which the outcomes of the various individuals come about through social interactions. When interference is present, causal inference is rendered considerably more complex, and the literature on causal inference in the presence of interference has just recently begun to develop. In this article we summarise some of the concepts and results from the existing literature and extend that literature in considering new results for finite sample inference, new inverse probability weighting estimators in the presence of interference and new causal estimands of interest.},
 author = {{Tchetgen Tchetgen}, Eric J. and VanderWeele, Tyler J.},
 year = {2012},
 title = {On causal inference in the presence of interference},
 keywords = {Biomedical Research/statistics {\&} numerical data;Causality;Communicable Diseases/epidemiology;Data Interpretation, Statistical;Humans;Models, Statistical;Randomized Controlled Trials as Topic/statistics {\&} numerical data;Vaccination/statistics {\&} numerical data},
 pages = {55--75},
 volume = {21},
 number = {1},
 journal = {Statistical Methods in Medical Research},
 doi = {10.1177/0962280210386779},
 file = {nihms632455:Attachments/nihms632455.pdf:application/pdf}
}


@inproceedings{Thomas.2016,
 abstract = {Proceedings of the International Conference on Machine Learning 2016},
 author = {Thomas, Phillip S. and Brunskill, Emma},
 title = {Data-efficient off-Policy policy evaluation for reinforcement Learning},
 keywords = {complex return;MAGIC estimator;MMSE return;off-policy;policy evaluation;Reinforcement Learning},
 booktitle = {ICML},
 year = {2016},
 file = {thomasa16:Attachments/thomasa16.pdf:application/pdf}
}


@article{Tomkins.2020,
 abstract = {In mobile health (mHealth), reinforcement learning algorithms that adapt to one's context without learning personalized policies might fail to distinguish between the needs of individuals. Yet the high amount of noise due to the in situ delivery of mHealth interventions can cripple the ability of an algorithm to learn when given access to only a single user's data, making personalization challenging. We present IntelligentPooling, which learns personalized policies via an adaptive, principled use of other users' data. We show that IntelligentPooling achieves an average of 26{\%} lower regret than state-of-the-art across all generative models. Additionally, we inspect the behavior of this approach in a live clinical trial, demonstrating its ability to learn from even a small group of users.},
 author = {Tomkins, Sabina and Liao, Peng and Klasnja, Predrag and Yeung, Serena and Murphy, Susan},
 title = {Rapidly personalizing mobile health treatment policies with limited data},
 url = {http://arxiv.org/pdf/2002.09971v1},
 keywords = {Computer Science - Computers and Society;Computer Science - Learning;Statistics - Machine Learning},
 journal = {arXiv preprint},
 file = {2002.09971:Attachments/2002.09971.pdf:application/pdf}
}


@article{Tran.2019,
 abstract = {A number of sophisticated estimators of longitudinal effects have been proposed for estimating the intervention-specific mean outcome. However, there is a relative paucity of research comparing these methods directly to one another. In this study, we compare various approaches to estimating a causal effect in a longitudinal treatment setting using both simulated data and data measured from a human immunodeficiency virus cohort. Six distinct estimators are considered: (i) an iterated conditional expectation representation, (ii) an inverse propensity weighted method, (iii) an augmented inverse propensity weighted method, (iv) a double robust iterated conditional expectation estimator, (v) a modified version of the double robust iterated conditional expectation estimator, and (vi) a targeted minimum loss-based estimator. The details of each estimator and its implementation are presented along with nuisance parameter estimation details, which include potentially pooling the observed data across all subjects regardless of treatment history and using data adaptive machine learning algorithms. Simulations are constructed over six time points, with each time point steadily increasing in positivity violations. Estimation is carried out for both the simulations and applied example using each of the six estimators under both stratified and pooled approaches of nuisance parameter estimation. Simulation results show that double robust estimators remained without meaningful bias as long as at least one of the two nuisance parameters were estimated with a correctly specified model. Under full misspecification, the bias of the double robust estimators remained better than that of the inverse propensity estimator under misspecification, but worse than the iterated conditional expectation estimator. Weighted estimators tended to show better performance than the covariate estimators. As positivity violations increased, the mean squared error and bias of all estimators considered became worse, with covariate-based double robust estimators especially susceptible. Applied analyses showed similar estimates at most time points, with the important exception of the inverse propensity estimator which deviated markedly as positivity violations increased. Given its efficiency, ability to respect the parameter space, and observed performance, we recommend the pooled and weighted targeted minimum loss-based estimator.},
 author = {Tran, Linh and Yiannoutsos, Constantin and Wools-Kaloustian, Kara and Siika, Abraham and {van der Laan}, Mark and Petersen, Maya},
 year = {2019},
 title = {Double robust efficient estimators of longitudinal treatment effects: Comparative performance in simulations and a case study},
 keywords = {Algorithms;Biostatistics/methods;Causality;Cohort Studies;Computer Simulation;Data Interpretation, Statistical;HIV Infections/mortality/therapy;Humans;Likelihood Functions;Longitudinal Studies;Machine learning;Models, Statistical;Probability;Propensity Score;Research Design;Software;Treatment Outcome},
 volume = {15},
 number = {2},
 journal = {The International Journal of Biostatistics},
 doi = {10.1515/ijb-2017-0054},
 file = {10.1515{\_}ijb-2017-0054:Attachments/10.1515{\_}ijb-2017-0054.pdf:application/pdf}
}


@proceedings{UAI.2007,
 year = {2007},
 title = {UAI}
}


@proceedings{UAI.2017,
 year = {2017},
 title = {UAI}
}


@article{vanderLaan.2006,
 abstract = {Suppose one observes a sample of independent and identically distributed observations from a particular data generating distribution. Suppose that one is concerned with estimation of a particular pathwise differentiable Euclidean parameter. A substitution estimator evaluating the parameter of a given likelihood based density estimator is typically too biased and might not even converge at the parametric rate: that is, the density estimator was targeted to be a good estimator of the density and might therefore result in a poor estimator of a particular smooth functional of the density. In this article we propose a one step (and, by iteration, k-th step) targeted maximum likelihood density estimator which involves 1) creating a hardest parametric submodel with parameter epsilon through the given density estimator with score equal to the efficient influence curve of the pathwise differentiable parameter at the density estimator, 2) estimating epsilon with the maximum likelihood estimator, and 3) defining a new density estimator as the corresponding update of the original density estimator. We show that iteration of this algorithm results in a targeted maximum likelihood density estimator which solves the efficient influence curve estimating equation and thereby yields a locally efficient estimator of the parameter of interest, under regularity conditions. In particular, we show that, if the parameter is linear and the model is convex, then the targeted maximum likelihood estimator is often achieved in the first step, and it results in a locally efficient estimator at an arbitrary (e.g., heavily misspecified) starting density.We also show that the targeted maximum likelihood estimators are now in full agreement with the locally efficient estimating function methodology as presented in Robins and Rotnitzky (1992) and van der Laan and Robins (2003), creating, in particular, algebraic equivalence between the double robust locally efficient estimators using the targeted maximum likelihood estimators as an estimate of its nuisance parameters, and targeted maximum likelihood estimators. In addition, it is argued that the targeted MLE has various advantages relative to the current estimating function based approach. We proceed by providing data driven methodologies to select the initial density estimator for the targeted MLE, thereby providing data adaptive targeted maximum likelihood estimation methodology. We illustrate the method with various worked out examples.},
 author = {{van der Laan}, Mark J. and Rubin, Donald B.},
 year = {2006},
 title = {Targeted maximum likelihood learning},
 volume = {2},
 number = {1},
 journal = {The International Journal of Biostatistics},
 file = {Daniel Rubin 1043 - Targeted Maximum Likelihood Learning:Attachments/Daniel Rubin 1043 - Targeted Maximum Likelihood Learning.pdf:application/pdf}
}


@article{vanderLaan.2007,
 abstract = {When trying to learn a model for the prediction of an outcome given a set of covariates, a statistician has many estimation procedures in their toolbox. A few examples of these candidate learners are: least squares, least angle regression, random forests, and spline regression. Previous articles (van der Laan and Dudoit (2003); van der Laan et al. (2006); Sinisi et al. (2007)) theoretically validated the use of cross validation to select an optimal learner among many candidate learners. Motivated by this use of cross validation, we propose a new prediction method for creating a weighted combination of many candidate learners to build the super learner. This article proposes a fast algorithm for constructing a super learner in prediction which uses V-fold cross-validation to select weights to combine an initial set of candidate learners. In addition, this paper contains a practical demonstration of the adaptivity of this so called super learner to various true data generating distributions. This approach for construction of a super learner generalizes to any parameter which can be defined as a minimizer of a loss function.},
 author = {{van der Laan}, Mark J. and Polley, Eric C. and Hubbard, Alan E.},
 year = {2007},
 title = {Super learner},
 pages = {1--23},
 volume = {6},
 journal = {Statistical Applications in Genetics and Molecular Biology},
 doi = {10.2202/1544-6115.1309}
}


@article{vanderLaan.2012,
 abstract = {We consider estimation of the effect of a multiple time point intervention on an outcome of interest, where the intervention nodes are subject to time-dependent confounding by intermediate covariates. In previous work van der Laan (2010) and Stitelman and van der Laan (2011a) developed and implemented a closed form targeted maximum likelihood estimator (TMLE) relying on the log-likelihood loss function, and demonstrated important gains relative to inverse probability of treatment weighted estimators and estimating equation based estimators. This TMLE relies on an initial estimator of the entire probability distribution of the longitudinal data structure. To enhance the finite sample performance of the TMLE of the target parameter it is of interest to select the smallest possible relevant part of the data generating distribution, which is estimated and updated by TMLE. Inspired by this goal, we develop a new closed form TMLE of an intervention specific mean outcome based on general longitudinal data structures. The target parameter is represented as an iterative sequence of conditional expectations of the outcome of interest. This collection of conditional means represents the relevant part, which is estimated and updated using the general TMLE algorithm. We also develop this new TMLE for other causal parameters, such as parameters defined by working marginal structural models. The theoretical properties of the TMLE are also practically demonstrated with a small scale simulation study.The proposed TMLE is building upon a previously proposed estimator Bang and Robins (2005) by integrating some of its key and innovative ideas into the TMLE framework.},
 author = {{van der Laan}, Mark J. and Gruber, Susan},
 year = {2012},
 title = {Targeted minimum loss based estimation of causal effects of multiple time point interventions},
 keywords = {Algorithms;Asymptotic linearity of an estimator;Bias;Biostatistics/methods;causal effect;Causality;confounding;efficient influence curve;G-computation formula;Humans;influence curve;Likelihood Functions;longitudinal data;Longitudinal Studies;loss function;marginal structural working model;Mathematical Concepts;Models, Statistical;Monte Carlo Method;nonparametric structural equation model;positivity a;Randomized Controlled Trials as Topic/statistics {\&} numerical data;Statistics, Nonparametric;Survival Analysis;Time Factors},
 volume = {8},
 number = {1},
 journal = {The International Journal of Biostatistics},
 doi = {10.1515/1557-4679.1370},
 file = {1557-4679.1370:Attachments/1557-4679.1370.pdf:application/pdf}
}


@book{vanderLaan.2018,
 author = {{van der Laan}, Mark J. and Rose, Sherri},
 year = {2018},
 title = {Targeted learning in data science},
 address = {Cham},
 publisher = {Springer},
 isbn = {978-3-319-65303-7},
 doi = {10.1007/978-3-319-65304-4},
 file = {Targeted Learning in Data Science by Mark J. van der Laan, Sherri Rose (z-lib.org):Attachments/Targeted Learning in Data Science by Mark J. van der Laan, Sherri Rose (z-lib.org).pdf:application/pdf}
}


@article{Vansteelandt.2006,
 author = {Vansteelandt, Stijn and Goetghebeur, Els and Kenward, Michael G. and Molenberghs, Geert},
 year = {2006},
 title = {Ignorance and uncertainty regions as inferential tools in a sensitivity analysis},
 pages = {953--979},
 volume = {16},
 journal = {Statistica Sinica},
 file = {A16n315:Attachments/A16n315.pdf:application/pdf}
}


@article{Vansteelandt.2016,
 author = {Vansteelandt, Stijn and Sjolander, Arvid},
 year = {2016},
 title = {Revisiting g-estimation of the effect of a time-varying exposure subject to time-varying confounding},
 volume = {5},
 number = {1},
 issn = {2194-9263},
 journal = {Epidemiologic Methods},
 doi = {10.1515/em-2015-0005},
 file = {revisiting{\_}gestimation:Attachments/revisiting{\_}gestimation.pdf:application/pdf}
}


@article{Varian.2016,
 abstract = {This is an elementary introduction to causal inference in economics written for readers familiar with machine learning methods. The critical step in any causal analysis is estimating the counterfactual-a prediction of what would have happened in the absence of the treatment. The powerful techniques used in machine learning may be useful for developing better estimates of the counterfactual, potentially improving causal inference.},
 author = {Varian, Hal R.},
 year = {2016},
 title = {Causal inference in economics and marketing},
 pages = {7310--7315},
 volume = {113},
 number = {27},
 journal = {Proceedings of the National Academy of Sciences (PNAS)},
 doi = {10.1073/pnas.1510479113},
 file = {Varian 2016 - Causal inference in economics:Attachments/Varian 2016 - Causal inference in economics.pdf:application/pdf}
}


@misc{Vergari.,
 author = {Vergari, Antonio and Choi, Yoojung and Peharz, Robert and {Van den Broeck}, Guy},
 title = {Probabilistic Circuits},
 file = {ECAI20:Attachments/ECAI20.pdf:application/pdf}
}


@misc{Wager.,
 author = {Wager, Stefan},
 title = {STATS 361: Causal Inference},
 file = {STATS 361 - Causal Inference:Attachments/STATS 361 - Causal Inference.pdf:application/pdf}
}


@article{Wager.2018,
 abstract = {Journal of the American Statistical Association, 2018. doi:10.1080/01621459.2017.1319839},
 author = {Wager, Stefan and Athey, Susan},
 year = {2018},
 title = {Estimation and inference of heterogeneous treatment effects using random forests},
 keywords = {Adaptive nearest neighbors matching;Asymptotic normality;Potential outcomes;Unconfoundedness},
 pages = {1228--1242},
 volume = {113},
 number = {523},
 journal = {Journal of the American Statistical Association},
 doi = {10.1080/01621459.2017.1319839},
 file = {Estimation and Inference of Heterogeneous Treatment Effects using Random Forests:Attachments/Estimation and Inference of Heterogeneous Treatment Effects using Random Forests.pdf:application/pdf}
}


@article{Wald.1940,
 author = {Wald, Abraham},
 year = {1940},
 title = {The fitting of straight lines if both variables are subject to error},
 pages = {284--300},
 volume = {11},
 number = {3},
 journal = {Annals of Mathematical Statistics},
 file = {1177731868:Attachments/1177731868.pdf:application/pdf}
}


@article{Wang.,
 abstract = {Instrumental variables are widely used to deal with unmeasured confounding in observational studies and imperfect randomized controlled trials. In these studies, researchers often target the so-called local average treatment effect as it is identifiable under mild conditions. In this paper, we consider estimation of the local average treatment effect under the binary instrumental variable model. We discuss the challenges for causal estimation with a binary outcome, and show that surprisingly, it can be more difficult than the case with a continuous outcome. We propose novel modeling and estimating procedures that improve upon existing proposals in terms of model congeniality, interpretability, robustness or efficiency. Our approach is illustrated via simulation studies and a real data analysis.},
 author = {Wang, Linbo and Zhang, Yuexia and Richardson, Thomas S. and Robins, James M.},
 title = {Estimation of local treatment effects under the binary instrumental  variable model},
 url = {http://arxiv.org/pdf/2007.14458v2},
 keywords = {Statistics - Methodology},
 issn = {0006-3444},
 journal = {Biometrika},
 file = {2007.14458:Attachments/2007.14458.pdf:application/pdf}
}


@article{Wang.2018,
 author = {Wang, Linbo and {Tchetgen Tchetgen}, Eric J.},
 year = {2018},
 title = {Bounded, efficient and multiply robust estimation of average treatment effects using instrumental variables},
 pages = {531--550},
 volume = {80},
 number = {3},
 issn = {1467-9868},
 journal = {Journal of the Royal Statistical Society: Series B},
 file = {1611.09925:Attachments/1611.09925.pdf:application/pdf}
}


@article{Wang.2019,
 abstract = {Causal inference from observational data is a vital problem, but it comes with strong assumptions. Most methods assume that we observe all confounders, variables that affect both the causal variabl...},
 author = {Wang, Yixin and Blei, David M.},
 year = {2019},
 title = {The blessings of multiple causes},
 pages = {1574--1596},
 volume = {114},
 number = {528},
 journal = {Journal of the American Statistical Association},
 doi = {10.1080/01621459.2019.1686987},
 file = {Wang, Blei 2019 - The Blessings of Multiple Causes:Attachments/Wang, Blei 2019 - The Blessings of Multiple Causes.pdf:application/pdf}
}


@inproceedings{Wang.2020,
 author = {Wang, Shirly and McDermott, Matthew B.A. and Chauhan, Geeticka and Ghassemi, Marzyeh and Hughes, Michael C. and Naumann, Tristan},
 title = {{MIMIC}-extract: A data extraction, preprocessing, and representation pipeline for {MIMIC-III}},
 booktitle = {CHIL},
 year = {2020},
 file = {MIMIC extract:Attachments/MIMIC extract.pdf:application/pdf}
}


@article{Wang.2021,
 abstract = {Management Science 0.0},
 author = {Wang, Guihua and Li, Jun and Hopp, Wallace J.},
 year = {2021},
 title = {An instrumental variable forest approach for detecting heterogeneous treatment effects in observational studies},
 keywords = {big data analytics;causal inference;heterogeneous treatment effects;Machine learning},
 issn = {0025-1909},
 journal = {Management Science},
 doi = {10.1287/mnsc.2021.4084},
 file = {mnsc.2021.4084:Attachments/mnsc.2021.4084.pdf:application/pdf}
}


@book{Wooldridge.2013,
 author = {Wooldridge, Jeffrey M.},
 year = {2013},
 title = {Introductory Econometrics: A modern approach},
 publisher = {Routledge},
 isbn = {9781136586101},
 doi = {10.4324/9780203157688},
 file = {Jeffrey{\_}M.{\_}Wooldridge{\_}Introductory{\_}Econometrics{\_}A{\_}Modern{\_}Approach{\_}{\_}2012:Attachments/Jeffrey{\_}M.{\_}Wooldridge{\_}Introductory{\_}Econometrics{\_}A{\_}Modern{\_}Approach{\_}{\_}2012.pdf:application/pdf}
}


@book{Wright.1928,
 author = {Wright, Phillip G.},
 year = {1928},
 title = {The tariff on animal and vegitable oils},
 address = {New York},
 publisher = {Macmillan}
}


@misc{Wu.12.06.2020,
 abstract = {The fundamental problem in treatment effect estimation from observational data is confounder identification and balancing. Most of the previous methods realized confounder balancing by treating all observed pre-treatment variables as confounders, ignoring further identifying confounders and non-confounders. In general, not all the observed pre-treatment variables are confounders that refer to the common causes of the treatment and the outcome, some variables only contribute to the treatment and some only contribute to the outcome. Balancing those non-confounders, including instrumental variables and adjustment variables, would generate additional bias for treatment effect estimation. By modeling the different causal relations among observed pre-treatment variables, treatment and outcome, we propose a synergistic learning framework to 1) identify confounders by learning decomposed representations of both confounders and non-confounders, 2) balance confounder with sample re-weighting technique, and simultaneously 3) estimate the treatment effect in observational studies via counterfactual inference. Empirical results on synthetic and real-world datasets demonstrate that the proposed method can precisely decompose confounders and achieve a more precise estimation of treatment effect than baselines.},
 author = {Wu, Anpeng and Kuang, Kun and Yuan, Junkun and Li, Bo and Wu, Runze and Zhu, Qiang and Zhuang, Yueting and Wu, Fei},
 date = {12.06.2020},
 title = {Learning Decomposed Representation for Counterfactual Inference},
 url = {http://arxiv.org/pdf/2006.07040v2},
 keywords = {Computer Science - Learning;Confounder Identification and Balancing;counterfactual inference;Decomposed Representation;Statistics - Machine Learning;Statistics - Methodology;Treatment Effect},
 file = {2006.07040:Attachments/2006.07040.pdf:application/pdf}
}


@inproceedings{Xu.2016,
 author = {Xu, Yanbo and Xu, Yanxun and Saria, Suchi},
 title = {A non-parametric bayesian approach for estimating treatment-response curves from sparse time series},
 booktitle = {ML4H},
 year = {2016}
}


@inproceedings{Xu.2021,
 abstract = {Instrumental variable (IV) regression is a standard strategy for learning causal relationships between confounded treatment and outcome variables from observational data by utilizing an instrumental variable, which affects the outcome only through the treatment. In classical IV regression, learning proceeds in two stages: stage 1 performs linear regression from the instrument to the treatment; and stage 2 performs linear regression from the treatment to the outcome, conditioned on the instrument. We propose a novel method, deep feature instrumental variable regression (DFIV), to address the case where relations between instruments, treatments, and outcomes may be nonlinear. In this case, deep neural nets are trained to define informative nonlinear features on the instruments and treatments. We propose an alternating training regime for these features to ensure good end-to-end performance when composing stages 1 and 2, thus obtaining highly flexible feature maps in a computationally efficient manner. DFIV outperforms recent state-of-the-art methods on challenging IV benchmarks, including settings involving high dimensional image data. DFIV also exhibits competitive performance in off-policy policy evaluation for reinforcement learning, which can be understood as an IV regression task.},
 author = {Xu, Liyuan and Chen, Yutian and Srinivasan, Siddarth and Freitas, Nando de and Doucet, Arnaud and Gretton, Arthur},
 title = {Learning deep features in instrumental variable regression},
 url = {http://arxiv.org/pdf/2010.07154v3},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {ICLR},
 year = {2021},
 file = {learning{\_}deep{\_}features{\_}in{\_}inst:Attachments/learning{\_}deep{\_}features{\_}in{\_}inst.pdf:application/pdf}
}


@article{Yang.2020,
 abstract = {Decision-makers often want to target interventions (e.g., marketing campaigns) so as to maximize an outcome that is observed only in the long-term. This typically requires delaying decisions until the outcome is observed or relying on simple short-term proxies for the long-term outcome. Here we build on the statistical surrogacy and off-policy learning literature to impute the missing long-term outcomes and then approximate the optimal targeting policy on the imputed outcomes via a doubly-robust approach. We apply our approach in large-scale proactive churn management experiments at The Boston Globe by targeting optimal discounts to its digital subscribers to maximize their long-term revenue. We first show that conditions for validity of average treatment effect estimation with imputed outcomes are also sufficient for valid policy evaluation and optimization; furthermore, these conditions can be somewhat relaxed for policy optimization. We then validate this approach empirically by comparing it with a policy learned on the ground truth long-term outcomes and show that they are statistically indistinguishable. Our approach also outperforms a policy learned on short-term proxies for the long-term outcome. In a second field experiment, we implement the optimal targeting policy with additional randomized exploration, which allows us to update the optimal policy for each new cohort of customers to account for potential non-stationarity. Over three years, our approach had a net-positive revenue impact in the range of {\$}4-5 million compared to The Boston Globe's current policies.},
 author = {Yang, Jeremy and Eckles, Dean and Dhillon, Paramveer and Aral, Sinan},
 year = {2020},
 title = {Targeting for long-term outcomes},
 url = {http://arxiv.org/pdf/2010.15835v1},
 keywords = {Computer Science - Learning;Statistics - Applications;Statistics - Machine Learning},
 journal = {arXiv preprint},
 file = {2010.15835:Attachments/2010.15835.pdf:application/pdf}
}


@article{Yazdani.2015,
 abstract = {Causal analyses and causal inference is a growing area of biostatics. In parallel, there is increasing focus on using genomic information to guide medical practice, i.e. personalized medicine or decision medicine. This perspective discusses causal inference in the context of personalized or decision medicine, including the assumptions and the concept that the task is different depending on whether the primary goal is the average response of treatment in the population or the ability to characterize the response for an individual or a subgroup. This perspective provides a tutorial of modern causal inference and then provides suggestions how application of specific kinds of causal inference would promote advances in translational sciences. The concept of the subpopulation causal effect is one path toward improved decision medicine. A dataset containing cardiovascular disease risk factor levels and genomic information is analyzed and different causal effects are estimated.},
 author = {Yazdani, Azam M. and Boerwinkle, Eric},
 year = {2015},
 title = {Causal inference in the age of decision medicine},
 volume = {6},
 number = {1},
 issn = {2153-0602},
 journal = {Journal of Data Mining in Genomics {\&} Proteomics},
 doi = {10.4172/2153-0602.1000163}
}


@inproceedings{Yoon.2018,
 author = {Yoon, Jinsung and Jordon, James and {van der Schaar}, Mihaela},
 title = {GANITE: Estimation of individualized treatment effects using generative adversarial nets},
 booktitle = {ICLR},
 year = {2018},
 file = {ganite{\_}estimation{\_}of{\_}individua:Attachments/ganite{\_}estimation{\_}of{\_}individua.pdf:application/pdf}
}


@inproceedings{Yu.2016,
 author = {Yu, Hsiang-Fu and Rao, Nikhil and Dhillon, Inderjit S.},
 title = {Temporal regularized matrix factorization for high-dimensional time series prediction},
 booktitle = {NeurIPS},
 year = {2016},
 file = {Yu, Rao et al. 2016 - Temporal Regularized Matrix Factorization:Attachments/Yu, Rao et al. 2016 - Temporal Regularized Matrix Factorization.pdf:application/pdf}
}


@inproceedings{Zhang.2020,
 abstract = {The choice of making an intervention depends on its potential benefit or harm in comparison to alternatives. Estimating the likely outcome of alternatives from observational data is a challenging problem as all outcomes are never observed, and selection bias precludes the direct comparison of differently intervened groups. Despite their empirical success, we show that algorithms that learn domain-invariant representations of inputs (on which to make predictions) are often inappropriate, and develop generalization bounds that demonstrate the dependence on domain overlap and highlight the need for invertible latent maps. Based on these results, we develop a deep kernel regression algorithm and posterior regularization framework that substantially outperforms the state-of-the-art on a variety of benchmarks data sets.},
 author = {Zhang, Yao and Bellot, Alexis and {van der Schaar}, Mihaela},
 title = {Learning overlapping representations for the estimation of  individualized treatment effects},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 booktitle = {AISTATS},
 year = {2020},
 file = {Learning Overlapping Representations:Attachments/Learning Overlapping Representations.pdf:application/pdf}
}


